---
title: "DC1_Part1"
author: "Laura Bozzi and Sarina Sägesser"
format: html
editor: visual
---

# DC1 Assignment, Part 1 (Spatial Clustering)

branch testing

```{r}
library(readr) 
library(tidyverse) 
library(tidyr) 
library(sf) 
library(dplyr)
library(ggplot2)
library(osmdata)
library(phonTools)

if(!"dbscan" %in% rownames(installed.packages())) install.packages("dbscan")
library(dbscan)
if(!"factoextra" %in% rownames(installed.packages())) install.packages("dbscan")
library(factoextra)

```

```{r}
crs_lv03  <- 21781 
crs_lv95  <- 2056 
crs_wgs84 <- 4326
```

## Task 1:

```{r}
accidents <- st_read("roadtrafficaccidentlocations.json", crs = 4326)
#head(accidents)
```

```{r}
plot(accidents)
```

## Task 2:

```{r}
#a. Number of accidents by accident severity category 
accident_severity <- accidents |>   
  group_by(AccidentSeverityCategory_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_severity
```

```{r}
#b. Number of accidents by accident type 
accident_type <- accidents |>   
  group_by(AccidentType_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_type
```

```{r}
#c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively 

# Still need to add the combinations of pedestrians, bicycles, and motorcycles
selected_involvements <- accidents |>   
  select(AccidentInvolvingPedestrian, AccidentInvolvingBicycle, AccidentInvolvingMotorcycle)  

accident_involvements <- selected_involvements |>   
  summarise(across(everything(), ~ sum(. == "true", na.rm = "true"))) %>%   
  bind_rows(selected_involvements %>%               
              summarise(across(everything(), ~ sum(. == "false", na.rm = "true"))))|>  
  st_drop_geometry()  

row.names(accident_involvements) <- c("true", "false")

accident_involvements
```

```{r}
#c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively 

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
all_three_involvements <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true" & 
         AccidentInvolvingMotorcycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

all_three_involvements

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
ped_bike_inv <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

ped_bike_inv

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
bike_moto_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

bike_moto_inv

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
moto_ped_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingPedestrian == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

moto_ped_inv
```

There seem to be no accidents where all three involvements (bike, motorcycle and pedestrian) were registered.

## Task 3:

```{r}
#accidents per year
accident_per_year <- accidents |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_total = n()) |>   
  st_drop_geometry()  
accident_per_year

#bicycle accidents per year
bicycle_accidents_per_year <- accidents |>
  filter(AccidentInvolvingBicycle == "true") |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_bicycle = n()) |>   
  st_drop_geometry()  
bicycle_accidents_per_year

#combine data
total_plus_bicycle_per_year = left_join(accident_per_year, bicycle_accidents_per_year, by = "AccidentYear")
total_plus_bicycle_per_year
```

```{r}
ggplot(accident_per_year, aes(x = accident_per_year$AccidentYear, y= accident_per_year$count_total))+
  geom_line(color = "steelblue", size = 1) +         
  geom_point(color = "steelblue", size = 2) +         
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents") +
  theme_minimal() 

ggplot(total_plus_bicycle_per_year, aes(x = AccidentYear)) +
  geom_line(aes(y = count_total, color = "Total Accidents"), size = 1) + 
  geom_point(aes(y = count_total, color = "Total Accidents"), size = 2) +
  geom_line(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 1) +
  geom_point(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 2) +
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents",
       colour = 'Legend') +
  scale_color_manual(values = c("Total Accidents" = "navyblue", "Bicycle Accidents" = "brown")) +
  theme_minimal()
```

## Task 4:

```{r}
bicycle_accidents <- accidents |>
  filter(AccidentInvolvingBicycle == "true")

bbox <- st_bbox(bicycle_accidents)

osm_basemap <- opq(bbox = bbox) |>
  add_osm_feature(key="boundary", value = "administrative") %>%
  add_osm_feature(key="admin_level", value = "8") %>%
  osmdata_sf()
```

```{r}
zh_city_boundary <- st_read('data/Zurich_city_boundary_2024.gpkg')
```

```{r}
ggplot() +
  geom_sf(data = zh_city_boundary) +
  #geom_sf(data = osm_basemap$osm_lines, color = "gray", size = 0.5, alpha = 0.7) +
  geom_sf(data = bicycle_accidents, aes(color = bicycle_accidents$AccidentSeverityCategory_en), size = 1.5, alpha = 0.8) +
  labs(title = "Accidents on OpenStreetMap Basemap",
       subtitle = "Accidents plotted over streets from OSM",
       color = "Accident Severity") +
  coord_sf() +
  theme_minimal() 
```

## Task 5:

```{r}
#Imagine you are given the task of detecting spatial clusters of elevated bicycle accident occurrence (without considering their severity). How would you characterize such “bicycle accident clusters”? Try to define properties that can be used to describe and identify such clusters, and that can be used to choose and parameterize a clustering method suitable for the task. Try to use natural, but precise and concise language in your answer.
```

| We would be interested in knowing in what areas of Zurich the most bicycle accidents happen. On the generated map of all bicycle accident one can see that in the city center, close to the main train station there are many accidents. Such accident herds could therefore be detected with more security with a spatial density clustering.
| The accidents should occur within close proximity to one another, which indicates a spatial dependence. A threshold for maximum distance between accidents would be used in this case, as well as the minimum number of points within a cluster.
| 
| Parameters: Distance from one point to another and the minimum number of accidents within the defined distance radius to form a cluster.

## Task 6:

### Preliminary work

```{r}
bicycle_crd <- sf::st_coordinates(bicycle_accidents)
```

```{r}
bicycle_accidents_2018 <- bicycle_accidents |>
  filter(AccidentYear == "2018") 

bicycle_accidents_2019 <- bicycle_accidents |>
  filter(AccidentYear == "2019") 

bicycle_accidents_2020 <- bicycle_accidents |>
  filter(AccidentYear == "2020") 

bicycle_accidents_2021 <- bicycle_accidents |>
  filter(AccidentYear == "2021") 
```

```{r}
bicycle_crd_2018 <- sf::st_coordinates(bicycle_accidents_2018)
bicycle_crd_2018 <- bicycle_crd_2018[, !(colnames(bicycle_crd_2018) %in% "Z")]

bicycle_crd_2019 <- sf::st_coordinates(bicycle_accidents_2019)
bicycle_crd_2019 <- bicycle_crd_2019[, !(colnames(bicycle_crd_2019) %in% "Z")]

bicycle_crd_2020 <- sf::st_coordinates(bicycle_accidents_2020)
bicycle_crd_2020 <- bicycle_crd_2020[, !(colnames(bicycle_crd_2020) %in% "Z")]

bicycle_crd_2021 <- sf::st_coordinates(bicycle_accidents_2021)
bicycle_crd_2021 <- bicycle_crd_2021[, !(colnames(bicycle_crd_2021) %in% "Z")]
```

```{r}
#Found this function when looking for a way to find the number of clusters. Source: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust
fviz_nbclust(bicycle_crd_2021, kmeans, method = "silhouette")
```

### Bicycle accident clusters 2018

```{r}
#| label: dbscan-bicycle-accidents
# 2018
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2018, k = 3)

# We can somehow see a knee at about 0.009 km.
abline(h = 0.009, col = "red")

# So, let's also try 0.005 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db18_01 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.009, minPts = 3)
db18_01
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_01$cluster + 1,
     main = "DBSCAN result with eps = 0.009 km",
     asp = 1)

db18_02 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.004, minPts = 3)
db18_02
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db18_03 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.002, minPts = 3)
db18_03
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2019

```{r}
#| label: dbscan-bicycle-accidents
# 2019
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2019, k = 3)

# We can somehow see a knee at about 0.014 km.
abline(h = 0.01, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db19_01 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.01, minPts = 3)
db19_01
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_01$cluster + 1,
     main = "DBSCAN result with eps = 0.01 km",
     asp = 1)

db19_02 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.004, minPts = 3)
db19_02
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db19_03 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.002, minPts = 3)
db19_03
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2020

```{r}
#| label: dbscan-bicycle-accidents
# 2020
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2020, k = 3)

# We can somehow see a knee at about 0.014 km.
abline(h = 0.014, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 0.005, col = "blue")

# Now compute DBSCAN with different values for eps.
db20_01 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.014, minPts = 3)
db20_01
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_01$cluster + 1,
     main = "DBSCAN result with eps = 0.014 km",
     asp = 1)

db20_02 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.005, minPts = 3)
db20_02
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_02$cluster + 1,
     main = "DBSCAN result with eps = 0.005 km",
     asp = 1)

db20_03 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.0025, minPts = 3)
db20_03
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result with eps = 0.0025 km",
     asp = 1)
```

### Bicycle accident clusters 2021

```{r}
#| label: dbscan-bicycle-accidents
# 2021
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2021, k = 3)

# We can somehow see a knee at about 0.012 km.
abline(h = 0.012, col = "red")

# So, let's also try  0.004 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db21_01 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.012, minPts = 3)
db21_01
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_01$cluster + 1,
     main = "DBSCAN result with eps = 0.012 km",
     asp = 1)

db21_02 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.003, minPts = 3)
db21_02
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_02$cluster + 1,
     main = "DBSCAN result with eps = 0.003 km",
     asp = 1)

db21_03 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.002, minPts = 3)
db21_03
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Trying OPTICS algorithm

Here, the help of ChatGPT was used to get the OPTICS algorithm right.

```{r}
# First run with eps = 0.05 and minPts = 3
db <- dbscan::optics(bicycle_crd_2018, eps = 0.05, minPts = 3)
db
clusters <- dbscan::extractXi(db, xi = 0.05)
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.05, minPts = 3)",
     asp = 1)

# Second run with eps = 0.015 and minPts = 5
db <- dbscan::optics(bicycle_crd_2018, eps = 0.015, minPts = 5)
db
clusters <- dbscan::extractXi(db, xi = 0.05)  # Re-extract clusters for this run
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.015, minPts = 5)",
     asp = 1)

```

## Task 7:

Discuss your results, including also limitations or problems, and possible other methods that you could have used.

| In the cluster diagrams of the years 2018-2021 there is not too much variation from one year to another. What stands out in every diagram, at least the two more accurate ones (with the two smallest eps distance) show the kernel around the main train station nicely. We could say that the Zurich HB/ Central acts as a bicycle accident herd. This is no surprise as many tram lines pass by the Zurich HB, as well as that is very often crowded with cars, pedestrians and cyclists.
| 

| The OPTICS algorithm has been used as a comparison. However, another method which could have been used was the convex cluster hulls. In some parts of Zurich that might have been an addition to the straight forward analysis but for example around HB we would have no holes within the spaces of large density.
| 
| What can be said as limiting factors is that the streets were not included in the calculation. It would be interesting to know if most accidents happen on main or peripheral streets. Moreover, the time has not been included either. We could filter for the rush hour in a next step, in order to analyse the accidents during that time. There, we might have other herds than when looking at all the accidents.

## Task 8:

### 8a)

| The method should be able to detect an area which is very dense in bicycle accidents. It should work kind of like a heat map, where we see a center where many accidents happen on the same spot and towards the edges there are less and less. Therefore, the polygon should be drawn at the border between "many accidents and less accidents".

### 8b)

| Based on the criteria above, we would chose the **Kernel Density Estimation (KDE) with bandwidth selection method HREF.** With this method, we could define clusters based on areas where KDE values exceed a certain threshold, indicating high accident concentration.
| As we don't have attributes which we want to use to cluster the accidents but only the densities on the map such a method would be suitable. However, we wonder if it's possible to generate this cluster estimation with more than one center.
| As an alternative, the method the Standard Deviation Ellipse will be tried out.
| 
| 

## Task 9:

*das une isch leider nid so vielussegend... vilicht wär KDE wük besser, aber chunnt ersch in vl 4 vor....*

```{r}

#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd <- phonTools::sdellipse(bicycle_crd_2018, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd[,1], y = sde_crd[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2018[,1]), y = mean(sde_crd[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2018")

```

```{r}
#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd <- phonTools::sdellipse(bicycle_crd_2019, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd[,1], y = sde_crd[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2019[,1]), y = mean(sde_crd[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2019")

```

### Jaccard Index:

```{r}
#In order to compute the jaccard index, the matrices have to become the same length, we therefore shorten all four years to the shortest of the four. 

# Find the minimum number of rows among all matrices
min_length <- min(nrow(bicycle_crd_2018), nrow(bicycle_crd_2019), nrow(bicycle_crd_2020), nrow(bicycle_crd_2021))

# Shorten all matrices to have the same number of rows as the smallest matrix
matrix18 <- bicycle_crd_2018[1:min_length, ]
matrix19 <- bicycle_crd_2019[1:min_length, ]
matrix20 <- bicycle_crd_2020[1:min_length, ]
matrix21 <- bicycle_crd_2021[1:min_length, ]

```

```{r}
#adehabitatHR::kerneloverlap()
install.packages("vegan")
library(vegan)
df<-data.frame(matrix18, matrix19)
vegdist(df, method = "jaccard")

```

```{r}
#Found this jaccard index function here: https://www.r-bloggers.com/2021/11/how-to-calculate-jaccard-similarity-in-r-2/
#Define the function:
jaccard <- function(a, b) {
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}

#Compute the similarity between the matrices of bike accidents in 2018 and 2019
jaccard(matrix18, matrix19)
```

## Task 10:

Trying to plot four graphs next to each other

```{r}
# 4 figures arranged in 2 rows and 2 columns
par(mfrow = c(2, 2),
    mar = c(4, 4, 1, 1),  # Margin sizes: c(bottom, left, top, right)
    oma = c(1, 1, 2, 1))  # Outer margin sizes: c(bottom, left, top, right)


# Plot for the year 2018
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km (2018)",
     asp = 1)

# Plot for the year 2019
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km (2019)",
     asp = 1)

# Plot for the year 2020
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result with eps = 0.0025 km (2020)",
     asp = 1)

# Plot for the year 2021
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km (2021)",
     asp = 1)

```

*Task 10: Overall, what did you find with the above steps? What do these steps tell you about
the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing
the given data? Any other points of note?*

| The above steps and graphs show us that the largest accident herd in the city of Zurich is between the upper end of lake Zurich and Letten. On all the graphs, a large blob of points can be seen. On the standard deviational ellipse we could also see this well.
| However, we cannot tell much about the other areas and streets around it as the point clusters don't seem like the best option to analyse such herds. This method with coloring the different clusters seems more adapt for datasets wich are to be clustered by classes. As we don't have classes here we want to distinguish but rather analyse denisties, a kernel density estimation would be of more interest. 
