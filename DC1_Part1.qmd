---
title: "DC1_Part1"
author: "Laura Bozzi and Sarina Sägesser"
format: html
editor: visual
---

# DC1 Assignment, Part 1 (Spatial Clustering)

## Preliminary work

### Downloading necessary packages

```{r}
check_pkg <- function(x)
  {
    if (!library(x, character.only = TRUE, logical.return = TRUE)) {
      install.packages(x, dep = TRUE)
      if (!library(x, character.only = TRUE, 
                  logical.return = TRUE)) stop("Package not found")
    }
}

check_pkg("sf")
check_pkg("readr")    
check_pkg("tidyverse")
check_pkg("tidyr")
check_pkg("dplyr")
check_pkg("ggplot2")
check_pkg("osmdata")
check_pkg("phonTools")
check_pkg("spdep")
check_pkg("dbscan")
check_pkg("factoextra")
check_pkg("spatstat")
check_pkg("leaflegend")
check_pkg("leaflet.extras")
check_pkg("leaflet")
check_pkg("httr")
check_pkg("httr2")
check_pkg("spdep")
check_pkg("tmap")
check_pkg("mapview")
```

### Defining relevant coordinate systems:

```{r}
crs_lv03  <- 21781 
crs_lv95  <- 2056 
crs_wgs84 <- 4326
```

## Task 1:

*Download and import the road accident data from Open Data Zurich (ODZ) using an appropriate file format. If needed, transform to the Swiss projected CH1903+/LV95 coordinate system (EPSG = 2056).*

```{r}
accidents <- st_read("roadtrafficaccidentlocations.gpkg")
zh_city_boundary <- st_read('data/Zurich_city_boundary_2024.gpkg')
zh_city_roads <- st_read('data/Strassennetz_-OGD.gpkg')
zh_city_districts <- st_read('data/Zurich_city_districts_2024.gpkg')
```

As the point dimension seems to have a Z coordinate which is always 0, we remove it here already.

```{r}
# Remove the Z coordinate from the geometry column
accidents <- sf::st_zm(accidents)

# Check if the Z coordinate has been successfully removed
print(accidents)
```

## Task 2:

*Report the following numbers in table(s):*

*a. Number of accidents by accident severity category*

```{r}
#Filtering by accident severity 
accident_severity <- accidents |>   
  group_by(AccidentSeverityCategory_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_severity
```

*b. Number of accidents by accident type*

```{r}
#Filtering by accident type 
accident_type <- accidents |>   
  group_by(AccidentType_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_type
```

*c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively. And combinations thereof (pedestrian AND bicycle, pedestrian AND motorcycle etc.). Are there any accidents involving all three modes (pedestrian, bicycle, motorcycle)?*

```{r}
#Filtering by accidents involving pedestrians, bicycles, and motorcycles, respectively 

selected_involvements <- accidents |>   
  dplyr::select(AccidentInvolvingPedestrian, AccidentInvolvingBicycle, AccidentInvolvingMotorcycle)  

accident_involvements <- selected_involvements |>   
  dplyr::summarise(across(everything(), ~ sum(. == "true", na.rm = "true"))) %>%   
  bind_rows(selected_involvements %>%               
              summarise(across(everything(), ~ sum(. == "false", na.rm = "true"))))|>  
  st_drop_geometry()  

row.names(accident_involvements) <- c("true", "false")

accident_involvements
```

```{r}
# Count accidents involving all three categories
all_three_involvements <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true" & 
         AccidentInvolvingMotorcycle == "true") |>
  st_drop_geometry() |>
  summarise(count = n()) |>
  mutate(type = "Pedestrian, Bicycle, Motorcycle")

# Count accidents involving pedestrians and bicycles
ped_bike_inv <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry() |>
  summarise(count = n()) |>
  mutate(type = "Pedestrian, Bicycle")

# Count accidents involving bicycles and motorcycles
bike_moto_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry() |>
  summarise(count = n()) |>
  mutate(type = "Bicycle, Motorcycle")

# Count accidents involving pedestrians and motorcycles
moto_ped_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingPedestrian == "true") |>
  st_drop_geometry() |>
  summarise(count = n()) |>
  mutate(type = "Pedestrian, Motorcycle")

# Combine all counts into a single dataframe
combined_counts <- bind_rows(all_three_involvements, ped_bike_inv, bike_moto_inv, moto_ped_inv)

# Display the combined dataframe
combined_counts
```

There are no accidents in the dataset where all three involvements (bike, motorcycle and pedestrian) were registered. However, the results of the combines categories pedestrian & bicycle stand out, having more than three times as many results as the combination bicycle & motorcycle as well as pedestrian & motorcycle.

## Task 3:

*Generate a plot showing the temporal evolution of the number of accidents from 2011 to 2023. Label each year with the corresponding number of accidents. Choose a plot type that is suitable for this type of temporal data. Bonus: Show also the data for the bicycle accidents (cf. Task 4) in the same plot.*

```{r}
#accidents per year
accident_per_year <- accidents |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_total = n()) |>   
  st_drop_geometry()

#bicycle accidents per year
bicycle_accidents_per_year <- accidents |>
  filter(AccidentInvolvingBicycle == "true") |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_bicycle = n()) |>   
  st_drop_geometry()  

#combine data
total_plus_bicycle_per_year = left_join(accident_per_year, bicycle_accidents_per_year, by = "AccidentYear")
total_plus_bicycle_per_year
```

```{r}
ggplot(accident_per_year, aes(x = AccidentYear, y= count_total))+
  geom_line(color = "steelblue", linewidth = 1) +         
  geom_point(color = "steelblue", size = 2) +         
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents") +
  theme_minimal() 

ggplot(total_plus_bicycle_per_year, aes(x = AccidentYear)) +
  geom_line(aes(y = count_total, color = "Total Accidents"), linewidth = 1) + 
  geom_point(aes(y = count_total, color = "Total Accidents"), size = 2) +
  geom_line(aes(y = count_bicycle, color = "Bicycle Accidents"), linewidth = 1) +
  geom_point(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 2) +
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents",
       colour = 'Legend') +
  scale_color_manual(values = c("Total Accidents" = "navyblue", "Bicycle Accidents" = "brown")) +
  theme_minimal()
```

## Task 4:

*Select only those accidents that involved a bicycle. From now on, and for the remainder of DC1, we will restrict our analysis to the accidents involving bicycles. With this subset, produce a map showing the bicycle accident data colored by accident severity category. Use a basemap such as OpenStreetMap and/or the boundary data available on OLAT, so the accidents can be visually and spatially referenced.*

```{r}
bicycle_accidents <- accidents |>
  filter(AccidentInvolvingBicycle == "true")

bbox <- st_bbox(bicycle_accidents)
```

```{r}
ggplot() +
  geom_sf(data = zh_city_boundary) +
  geom_sf(data = bicycle_accidents, aes(color = AccidentSeverityCategory_en), size = 1.5, alpha = 0.8) +
  labs(title = "Accidents in the city of Zurich",
       subtitle = "Accidents coloured by severity",
       color = "Accident Severity") +
  coord_sf() +
  theme_minimal() 
```

Additionally to the basic visualisation above, we played around with leaflet maps (ChatGPT corrected our code however)

```{r}
#needed: library(leaflet)
#library(sf)

# Transform current data to WGS84 for use in leaflet
boundaries_distr_leaflet <- st_transform(zh_city_districts, "WGS84")
bicycle_accidents_leaflet <- st_transform(bicycle_accidents, "WGS84")

# Define a colour palette specific to the severity categories
severity_levels <- unique(bicycle_accidents$AccidentSeverityCategory_en)
color_palette <- colorFactor(palette = "Set1", domain = severity_levels)

# Create leaflet map
map <- leaflet() %>%
  setView(lng = 8.5219, lat = 47.3769, zoom = 11) %>%
  addProviderTiles(provider = providers$CartoDB.Positron) %>%
  addPolygons(data = boundaries_distr_leaflet, fill = FALSE, color = "black", weight = 1) %>%
  addSearchOSM() %>%
  setMaxBounds(lng1 = 8.5219 - 0.3, lat1 = 47.3769 - 0.3, lng2 = 8.5219 + 0.3, lat2 = 47.3769 + 0.3) %>%
  addCircles(data = bicycle_accidents_leaflet,
             color = ~color_palette(AccidentSeverityCategory_en),
             radius = 10,
             fill = TRUE,
             fillOpacity = 0.9,
             opacity = 1,
             stroke = FALSE,
             popup = ~AccidentSeverityCategory_en) %>% 
  addControl("Accident Types in Zurich, Switzerland", position = "topright") %>%
  addLegend(position = "topright",
            colors = color_palette(severity_levels),
            labels = severity_levels,
            title = "Accident Severity") %>%
  #addLayersControl(overlayGroups = c("Legend"),
                   #options = layersControlOptions(collapsed = FALSE)) %>%
  addFullscreenControl() %>%
  addScaleBar(position = "bottomleft", options = scaleBarOptions(imperial = FALSE))

map

```

## Task 5:

*Imagine you are given the task of detecting spatial clusters of elevated bicycle accident occurrence (without considering their severity). How would you characterize such “bicycle accident clusters”? Try to define properties that can be used to describe and identify such clusters, and that can be used to choose and parameterize a clustering method suitable for the task. Try to use natural, but precise and concise language in your answer.*

| We would be interested in knowing in what areas of Zurich the most bicycle accidents happen. On the generated map of all bicycle accident one can see that in the city center, close to the main train station there are many accidents. Such accident herds could therefore be detected with more security with a spatial density clustering.
| The accidents should occur within close proximity to one another, which indicates a spatial dependence. A threshold for maximum distance between accidents would be used in this case, as well as the minimum number of points within a cluster.
| 
| Parameters: Distance from one point to another and the minimum number of accidents within the defined distance radius to form a cluster.

## Task 6:

*From the bicycle accidents, extract the years 2018 to 2021 and compute clusters for each year separately, using a clustering method you deem appropriate for the task, and choose the control parameters appropriately to capture the types of clusters you had in mind in your definition of Task 5. Justify your choice.*

| We try to extract the clusters by using DBSCAN, as the method is a broadly used algorithm to detect spatial clusters. We apply this, because we have clusters of arbitrary shapes and large dataframes. Moreover, rather than looking at a certain attribute, we are interested in the spatial component, which can be analysed well with DBSCAN.

### Preliminary work

```{r}
bicycle_crd <- bicycle_accidents[, c("AccidentLocation_CHLV95_N", "AccidentLocation_CHLV95_E")]
```

```{r}
bicycle_accidents_2018 <- bicycle_accidents |>
  filter(AccidentYear == "2018") 

bicycle_accidents_2019 <- bicycle_accidents |>
  filter(AccidentYear == "2019") 

bicycle_accidents_2020 <- bicycle_accidents |>
  filter(AccidentYear == "2020") 

bicycle_accidents_2021 <- bicycle_accidents |>
  filter(AccidentYear == "2021") 
```

```{r}
#keep only X and Y coordinates for the further analysis
bicycle_crd_2018 <- sf::st_coordinates(bicycle_accidents_2018) 

bicycle_crd_2019 <- sf::st_coordinates(bicycle_accidents_2019) 

bicycle_crd_2020 <- sf::st_coordinates(bicycle_accidents_2020) 

bicycle_crd_2021 <- sf::st_coordinates(bicycle_accidents_2021)
```

```{r}
#Found this function when looking for a way to find the number of clusters. Source: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust
fviz_nbclust(bicycle_crd_2021, kmeans, method = "silhouette")
```

### Bicycle accident clusters 2018

```{r}
#| label: dbscan-bicycle-accidents
# 2018
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2018, k = 3)

# We can somehow see a knee at about 0.009 km.
abline(h = 900, col = "red")

# So, let's also try 0.005 km.
graphics::abline(h = 400, col = "blue")

# Now compute DBSCAN with different values for eps.
db18_01 <- dbscan::dbscan(bicycle_crd_2018, eps = 900, minPts = 3)
db18_01
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_01$cluster + 1,
     main = "DBSCAN result with eps = 0.009 km",
     asp = 1)

db18_02 <- dbscan::dbscan(bicycle_crd_2018, eps = 400, minPts = 3)
db18_02
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db18_03 <- dbscan::dbscan(bicycle_crd_2018, eps = 200, minPts = 3)
db18_03
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2019

```{r}
#| label: dbscan-bicycle-accidents
# 2019
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2019, k = 3)

# We can somehow see a knee at about 
abline(h = 800, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 400, col = "blue")

# Now compute DBSCAN with different values for eps.
db19_01 <- dbscan::dbscan(bicycle_crd_2019, eps = 800, minPts = 3)
db19_01
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_01$cluster + 1,
     main = "DBSCAN result with eps = 0.01 km",
     asp = 1)

db19_02 <- dbscan::dbscan(bicycle_crd_2019, eps = 400, minPts = 3)
db19_02
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db19_03 <- dbscan::dbscan(bicycle_crd_2019, eps = 200, minPts = 3)
db19_03
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2020

```{r}
#| label: dbscan-bicycle-accidents
# 2020
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2020, k = 3)

# We can somehow see a knee at about 0.04 km.
abline(h = 1000, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 400, col = "blue")

# Now compute DBSCAN with different values for eps.
db20_01 <- dbscan::dbscan(bicycle_crd_2020, eps = 1000, minPts = 3)
db20_01
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_01$cluster + 1,
     main = "DBSCAN result with eps = 0.014 km",
     asp = 1)

db20_02 <- dbscan::dbscan(bicycle_crd_2020, eps = 400, minPts = 3)
db20_02
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_02$cluster + 1,
     main = "DBSCAN result with eps = 0.005 km",
     asp = 1)

db20_03 <- dbscan::dbscan(bicycle_crd_2020, eps = 250, minPts = 3)
db20_03
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result with eps = 0.0025 km",
     asp = 1)
```

### Bicycle accident clusters 2021

```{r}
#| label: dbscan-bicycle-accidents
# 2021
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2021, k = 3)

# We can somehow see a knee at about 0.012 km.
abline(h = 800, col = "red")

# So, let's also try  0.004 km.
graphics::abline(h = 400, col = "blue")

# Now compute DBSCAN with different values for eps.
db21_01 <- dbscan::dbscan(bicycle_crd_2021, eps = 800, minPts = 3)
db21_01
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_01$cluster + 1,
     main = "DBSCAN result with eps = 0.012 km",
     asp = 1)

db21_02 <- dbscan::dbscan(bicycle_crd_2021, eps = 400, minPts = 3)
db21_02
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_02$cluster + 1,
     main = "DBSCAN result with eps = 0.003 km",
     asp = 1)

db21_03 <- dbscan::dbscan(bicycle_crd_2021, eps = 200, minPts = 3)
db21_03
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Trying OPTICS algorithm

Here, the help of ChatGPT was used to get the OPTICS algorithm right.

```{r}
# First run with eps = 0.05 and minPts = 3
db <- dbscan::optics(bicycle_crd_2018, eps = 500, minPts = 3)
db
clusters <- dbscan::extractXi(db, xi = 0.05)
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.05, minPts = 3)",
     asp = 1)

# Second run with eps = 0.015 and minPts = 5
db <- dbscan::optics(bicycle_crd_2018, eps = 150, minPts = 5)
db
clusters <- dbscan::extractXi(db, xi = 0.05)  # Re-extract clusters for this run
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.015, minPts = 5)",
     asp = 1)

```

## Task 7:

*Discuss your results, including also limitations or problems, and possible other methods that you could have used.*

| In the cluster diagrams of the years 2018-2021 there is not too much variation from one year to another. What stands out in every diagram, at least the two more accurate ones (with the two smallest eps distance) show the kernel around the main train station nicely. We could say that the Zurich HB/ Central acts as a bicycle accident herd. This is no surprise as many tram lines pass by the Zurich HB, as well as that is very often crowded with cars, pedestrians and cyclists.
| 

| The OPTICS algorithm has been used as a comparison. However, another method which could have been used was the convex cluster hulls. In some parts of Zurich that might have been an addition to the straight forward analysis but for example around HB we would have no holes within the spaces of large density.
| 
| What can be said as limiting factors is that the streets were not included in the calculation. It would be interesting to know if most accidents happen on main or peripheral streets. Moreover, the time has not been included either. We could filter for the rush hour in a next step, in order to analyse the accidents during that time. There, we might have other herds than when looking at all the accidents.

## Task 8:

*Given the clusters that you have extracted in Part 1 of the DC1 assignment: a. Define a set of criteria that a method should fulfill that can be used to delineate the given clusters by polygons. Use free text for these definitions, but try to be concise and precise. (Note: These criteria can also be used in the subsequent Discussion to evaluate whether they have been met.) b. Choose a polygon delineation method that you deem appropriate in light of the above criteria. Justify your choice.*

### 8a)

| The method should be able to detect an area which is very dense in bicycle accidents. It should work kind of like a heat map, where we see a center where many accidents happen on the same spot and towards the edges there are less and less. Therefore, the polygon should be drawn at the border between "many accidents and less accidents".

### 8b)

| Based on the criteria above, we would chose the **Kernel Density Estimation (KDE) with bandwidth selection method HREF.** With this method, we could define clusters based on areas where KDE values exceed a certain threshold, indicating high accident concentration.
| As we don't have attributes which we want to use to cluster the accidents but only the densities on the map such a method would be suitable. However, we wonder if it's possible to generate this cluster estimation with more than one center.
| As an alternative, the method the Standard Deviation Ellipse will be tried out.
| 
| 

## Task 9:

*From the years 2018 to 2021 for which you computed clusters in Task 6, choose at least two years and apply your polygon delineation method of choice to each of these two years separately. Compute the Jaccard Index (aka Intersection over Union) for pair(s) of selected years and present and discuss the results.*

```{r}
# OPTICS with the extractXi() function on the data
#2018
pt_bike18_reach <- dbscan::optics(bicycle_crd_2018, eps = 500, minPts = 10)
pt_bike18_reach <- dbscan::extractXi(pt_bike18_reach, xi = 0.05)
# Extract reachability distances from the OPTICS object
reachdist <- pt_bike18_reach$reachdist

# Plot reachability distances with specific ylim settings
plot(reachdist, type = "l", ylim = c(0,700), xlab = "Points", ylab = "Reachability Distance",main = "Reachability Plot 2018")
 
dbscan::hullplot(bicycle_crd_2018, pt_bike18_reach, asp = 1, main = "Convex Hull Plot 2018")

#2019
pt_bike19_reach <- dbscan::optics(bicycle_crd_2019, eps = 500, minPts = 3)
pt_bike19_reach <- dbscan::extractXi(pt_bike19_reach, xi = 0.05)
# Extract reachability distances from the OPTICS object
reachdist <- pt_bike19_reach$reachdist

# Plot reachability distances with specific ylim settings
plot(reachdist, type = "l", ylim = c(0,700), xlab = "Points", ylab = "Reachability Distance",
     main = "Reachability Plot 2019")
 
dbscan::hullplot(bicycle_crd_2019, pt_bike19_reach, asp = 1, main = "Convex Hull Plot 2019")
```

```{r}
summary(reachdist)
```

## Task 9 (alternative):

```{r}

#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd_18 <- phonTools::sdellipse(bicycle_crd_2018, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd_18[,1], y = sde_crd_18[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2018[,1]), y = mean(sde_crd_18[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2018")

```

```{r}
#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd_19 <- phonTools::sdellipse(bicycle_crd_2019, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd_19[,1], y = sde_crd_19[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2019[,1]), y = mean(sde_crd_19[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2019")

```

### Jaccard Index:

```{r}
#In order to compute the jaccard index, the matrices have to become the same length, we therefore shorten all four years to the shortest of the four. 

# Find the minimum number of rows among all matrices
min_length <- min(nrow(bicycle_crd_2018), nrow(bicycle_crd_2019), nrow(bicycle_crd_2020), nrow(bicycle_crd_2021))

# Shorten all matrices to have the same number of rows as the smallest matrix
matrix18 <- bicycle_crd_2018[1:min_length, ]
matrix19 <- bicycle_crd_2019[1:min_length, ]
matrix20 <- bicycle_crd_2020[1:min_length, ]
matrix21 <- bicycle_crd_2021[1:min_length, ]

```

```{r}
#adehabitatHR::kerneloverlap()
check_pkg("vegan")
df_18_19 <-data.frame(matrix18, matrix19)
vegdist(df_18_19, method = "jaccard")

```

```{r}
#Found this jaccard index function here: https://www.r-bloggers.com/2021/11/how-to-calculate-jaccard-similarity-in-r-2/
#Define the function:
jaccard <- function(a, b) {
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}

#Compute the similarity between the matrices of bike accidents in 2018 and 2019
jaccard(matrix18, matrix19)
#Compute the similarity between the delineated polygons of bike accidents in 2018 and 2019 --> maybe this is better (nr makes more sense) than matrices?? I guess for jaccard you need a polygon geometry and not points but unsure
jaccard(pt_bike18_reach, pt_bike19_reach)
```

| A value of 0.25 between the years 2018 and 2019 shows that the delineated polygons overlap with each other indeed but not too much. 0 means no intersection at all, whereby 1 would mean one area is covered by another, thus, they would have the same clusters and the same sizes in the two analysed years. We don't have the latter case, which can also be observer in the convex hull plots at the start of task 9. We assume however, that the main clusters do overlap, which is what makes out the above Jaccard index's score.

## Task 10:

*Overall, what did you find with the above steps? What do these steps tell you about the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing the given data? Any other points of note?*

Plotting four graphs next to each other for easier comparison:

```{r}
# 4 figures arranged in 2 rows and 2 columns
par(mfrow = c(2, 2),
    mar = c(4, 4, 1, 1),  # Margin sizes: c(bottom, left, top, right)
    oma = c(1, 1, 2, 1))  # Outer margin sizes: c(bottom, left, top, right)


# Plot for the year 2018
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2018)",
     asp = 1)

# Plot for the year 2019
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2019)",
     asp = 1)

# Plot for the year 2020
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result eps = 0.0025 km (2020)",
     asp = 1)

# Plot for the year 2021
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2021)",
     asp = 1)

```

| The above steps and graphs show us that the largest accident herd in the city of Zurich is between the upper end of lake Zurich and Letten. On all the graphs, a large blob of points can be seen. On the standard deviational ellipse we could also see this well.
| However, we cannot tell much about the other areas and streets around it as the point clusters don't seem like the best option to analyse such herds. This method with coloring the different clusters seems more adapt for datasets wich are to be clustered by classes. As we don't have classes here we want to distinguish but rather analyse densities, a kernel density estimation would be of more interest.

## Task 11

*Similarly to the clustering and polygon delineation tasks carried out in Parts 1 and 2 of DC1, respectively, start off by defining **criteria for using KDE to detect areas/hotspots of elevated bicycle accident density**, and explain your reasoning.*

Bandwidth selection and the kernel shape are the key criteria to define for using KDE.

Bandwidth h will be tested with HREF, LSCV as well as HPI. It would be interesting to use a weighted bandwidth, however, we don't have a value/location kind of column (like there is in the ungulates sample data). We tried to compute this but as no two accidents seem to be at the exact (important!) coordinates, it would have involved to create some kind of a buffer, then group and weigh the accidents per coordinates group... which would have been too time consuming.

We work with the bivariate normal kernel, which is the standard in kernelUD{adehabitatHR} and generally a popular option.

## Task 12

*Choose any two years from the years 2018 to 2021 (justify your choice of years) and compute the KDE surfaces for each of these two separately and visualize your results. You are free to choose the KDE implementation (i.e., R package and function(s)) as well as the parameters (bandwidth selection method, etc.), but you should document your choices and discuss, in the subsequent Task 14, your results in light of your choices.*

```{r}
#installing packages adehabitatHR and ks for KDE
check_pkg("adehabitatHR")
check_pkg("ks")
```

```{r}
#| label: kde-adehabitatHR
#| warning: false

# library(adehabitatHR)

ext_val <- 0.3    # 0.3
grid_val <- 300   # 300

# Convert the SF_PPOLYGON object to an SpatialPointsDataFrame object,
# because adehabitatHR wants SP objects as input.
# We use the as() function from the sf package.

# The inverse operation is done through `st_as_sf()` or `st_as_sfc()`, respectively. See the this [vignette](https://r-spatial.github.io/sf/articles/sf2.html) for examples (scroll to the bottom of the page).

bicycle_crds_18_sp <- as(bicycle_accidents_2018, "Spatial")

# First, use the reference bandwidth method for bandwidth selection (h = "href")
# Positioning of legend box is optimized for knitr HTML output

# Compute UD (utilization distribution)
ud <- adehabitatHR::kernelUD(bicycle_crds_18_sp, grid = grid_val, extent = ext_val, h = "href")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]",
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud@coords[,1])
xmax <- max(ud@coords[,1])
ymin <- min(ud@coords[,2])
ymax <- max(ud@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method HREF", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), 
       col = c("black", "blue"), lwd = c(3, 2), lty = c(4, 1), 
       inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with HREF (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with HREF (50 %): ", hr50$area, sep = "", "\n")
################################################################################

# Now we use the reference least-squares cross-validation method (h = "LSCV")
# Positioning of legend box is optimized for knitr HTML output
ud <- adehabitatHR::kernelUD(bicycle_crds_18_sp, grid = grid_val, extent = ext_val, h = "LSCV")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]", 
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud@coords[,1])
xmax <- max(ud@coords[,1])
ymin <- min(ud@coords[,2])
ymax <- max(ud@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method LSCV", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), col = c("black", "blue"), 
       lwd = c(3, 2), lty = c(4, 1), inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with LSCV (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with LSCV (50 %): ", hr50$area, sep = "", "\n")
```

| The problem of over-smoothing can be observed in the kernel density estimation with HREF, however, the estimation computed with least-squares cross validation (LSCV) is too crowded and thus not very useful to analyse our accident data. The KDE with HREF diagram is better to analyse the extend of the overall accidents, plus it shows where 50% of all the bike accidents (in the year 2018) were located. What is more, the 95% line also shows the outline of the lake and hill of Zurich, where expectedly no accidents happen.

```{r}
#Shorten the bicycle_accidents_2018 to the same length as matrix 18 has, namely 632 rows
# Shorten the sf table to the first 632 rows
bicycle_accidents_2018 <- bicycle_accidents_2018[1:632, ]

# Check if the sf table now has 632 rows
nrow(bicycle_accidents_2018)
```

```{r}
#| label: kde-ks
#| warning: false

# library(ks)

# First estimate a value for the bandwidth, using the "Plug-in" selector (Hpi).
# Needs plain coordinates matrix as input --> ung_crd
h <- ks::Hpi(x = matrix18)

# Now compute KDE; first without weights applied
fkde <- ks::kde(matrix18, H = h)

# Plotting utilizes the specialized S3 plot.kde() function
plot(fkde, display = "filled.contour2", 
     cont = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100),  # percentage contours
     main = "Unweighted KDE; plug-in bandwidth", asp = 1)
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)

# unweighted, with SCV (smoothed cross validation) bandwidth selector
h <- ks::Hscv(x = matrix18)
fkde <- ks::kde(matrix18, H = h)
plot(fkde, display = "persp",
     main = "Unweighted KDE; smoothed cross validation bandwidth (SCV)")
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)

# unweighted, with LSCV (least-squares cross validation) bandwidth selector
h <- ks::Hlscv(x = matrix18)
fkde <- ks::kde(matrix18, H = h)
plot(fkde, display = "image",
     main = "Unweighted KDE; least-squares cross validation bandwidth (LSCV)", asp = 1)
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)
```

```{r}
length(bicycle_accidents_2018$value) == nrow(matrix18)
```

## Task 12 alternative

```{r}
check_pkg("stats")

# stats::density() also requires a PPP object as input

X_crd <- as.numeric(bicycle_accidents$AccidentLocation_CHLV95_E)
Y_crd <- as.numeric(bicycle_accidents$AccidentLocation_CHLV95_N)

win_all <- as.owin(st_bbox(bicycle_accidents))
bicycle_crd_ppp <- spatstat.geom::ppp(x = X_crd, y = Y_crd, window = win_all, unitname = c("meter", "meters"))

summary(bicycle_crd_ppp)

# We use the default color ramp of the base plot function for display.
# Use the sigma argument for manual bandwidth selection.
plot(stats::density(bicycle_crd_ppp, sigma = 250), 
     main = "Bicycle accidents in Zurich, with density() and bandwidth sigma = 250")
#plot(bicycle_accidents, add = TRUE, pch = 16, cex = 0.5, col = "white")

```

```{r}
# Generate the KDE and convert to raster
density_ppp <- stats::density(bicycle_crd_ppp, sigma = 250)
density_raster <- raster::raster(density_ppp)

# Set the CRS for the raster to WGS84 for using it with mapview
raster::crs(density_raster) <- crs_lv95 #"+proj=longlat +datum=WGS84 +no_defs"

# Save the raster as a PNG with transparency
png_file <- "data/density_overlay.png"  # Specify path for saving
png(png_file, width = 800, height = 800, bg = "transparent")
plot(density_raster, col = hcl.colors(100, "YlOrRd", alpha = 0.7), legend = FALSE, axes = FALSE)
dev.off()

```

```{r}
### mapview package needed
#gebastelt with this source: https://bookdown.org/nicohahn/making_maps_with_r5/docs/mapview.html

check_pkg("png")
png_file <- readPNG('data/density_overlay.png')

mapview::mapview(quart_sf, alpha.regions = 0.3) +
  mapview(density_raster)
```

```{r}
# With the bw argument, automatic bandwidth selectors can be chosen:
# nrd0 = Silverman's rule of thumb; nrd = variation by Scott (1992)
# ucv, bcv = un/biased cross-validation
# SJ-ste = Sheather & Jones (1991) "solve the equation" method
# SJ-dpi = Sheather & Jones (1991) "direct plug-in" method
den_bcv <- stats::density(bicycle_crd_ppp, bw = "bcv")
plot(den_bcv, 
     main = "Ungulates in SNP, with stats::density() and bw selector = BCV")
#plot(bicycle_accidents, add = TRUE, pch = 16, cex = 0.5, col = "white")
```

## Task 13

*Compute the “volume of intersection” (“VI”) between the KDE surfaces (utilization distributions) of the two years. Hint: There are different ways to do that, but the adehabitatHR package has functionality for that. How do the results correspond to those of Task 9 (Jaccard Index or IoU)?*

```{r}
#theory:
#https://rdrr.io/cran/adehabitatHR/man/kerneloverlap.html

#kernel_18_VI <- kerneloverlap(df_18_19, method = "VI", percent = 95, conditional = FALSE)
#plot(kernel_18_VI)
```

```{r}
str(df_18_19)
```

```{r}
# Extract the coordinates from the sf object into a matrix
coords_matrix <- sf::st_coordinates(bicycle_accidents_2018)

# Display the column names of the coordinates matrix to verify
print(colnames(coords_matrix))

# Now select only the relevant columns (usually labeled as X and Y) for conversion
coords_matrix_xy <- coords_matrix[, c("X", "Y")]

# Convert the coordinates to a SpatialPoints object
spatial_points <- SpatialPoints(coords_matrix_xy, proj4string = CRS("+proj=longlat +datum=WGS84"))

head(spatial_points)

# Use the SpatialPoints object in the kerneloverlap function
#kernel_18_VI <- kerneloverlap(spatial_points, method = "VI", percent = 95, conditional = FALSE)
#this gives always the same error and Idk why?!

```

## Task 14

*Discuss your results for this part of DC1 (density estimation). What did you find? Compare the results of this part with the clusters/polygons of Parts 1 and 2 (see note below): What are the commonalities? What are the differences? Which method(s) perform more ade- quately than others for the given problem and data? Which method(s) would you recommend, and which ones not? Why? (You are free to add more points to the discussion.)*

| blabla
| 

## Task 15

*Choose one or more distance measure functions (justify your choice) and compute it/them for a. all bicycle accidents (2011 - 2021) b. bicycle accidents for one of the years selected in Task 6 c. bicycle accidents for only the cluster points of the year selected in (b) Restrict your analysis to the “inner city” and use the same window for all point sets. It’s up to you to define the extent of the “inner city” and explain/justify what that means in terms of this data challenge.*

### a)

```{r}
X_crd <- as.numeric(bicycle_accidents$AccidentLocation_CHLV95_E)
Y_crd <- as.numeric(bicycle_accidents$AccidentLocation_CHLV95_N)

win_all <- as.owin(st_bbox(bicycle_accidents))
bicycle_crd_ppp <- spatstat.geom::ppp(x = X_crd, y = Y_crd, window = win_all, unitname = c("meter", "meters"))

summary(bicycle_crd_ppp)

# Compute G-function using Kest(), with edge correction.
bicycle_crd_gf <- spatstat.explore::envelope(bicycle_crd_ppp, fun = Gest, nsim = 100, correction = "best", verbose = FALSE)

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd_gf, main = "G-function for all bicycle accidents points")
```

### b)

```{r}
win <- as.owin(st_bbox(bicycle_accidents_2018))
bicycle_crd18_ppp <- spatstat.geom::ppp(x = bicycle_crd_2018[,1], y = bicycle_crd_2018[,2], window = win, unitname = c("meter", "meters"))
```

```{r}
#| label: G-fun
#| 
# Compute G-function using Kest(), with edge correction.
bicycle_crd18_gf <- spatstat.explore::envelope(bicycle_crd18_ppp, fun = Gest, nsim = 100, correction = "best", verbose = FALSE)

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd18_gf, main = "G-function for bicycle accidents 2018 points")
```

### c)

```{r}
# connect the district with the bicycle_accidents
inner_city <- zh_city_districts |>
  filter(KNR == "1" )

inner_city_p <- st_join(bicycle_accidents_2018, inner_city)
inner_city_p <- na.omit(inner_city_p)
inner_city_p_xy <- as.data.frame(st_coordinates(inner_city_p))
```

```{r}
win <- as.owin(st_bbox(inner_city_p))
inner_18_ppp <- spatstat.geom::ppp(x = inner_city_p_xy$X, y = inner_city_p_xy$Y, window = win, unitname = c("meter", "meters"))

# Compute G-function using Gest(), with edge correction.
inner_18_ppp_gf <- spatstat.explore::envelope(inner_18_ppp, fun = Gest, nsim = 100, correction = "best", verbose = FALSE)

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(inner_18_ppp_gf, main = "G-function for inner city accidents in 2018")
```

## Task 16

*Now choose the following two pairs of years, 2018 & 2019, as well as 2018 & 2021, and compute the cross-X function, where “X” stands for the function(s) you used in Task 15. Use the AccidentYear as the marks to produce a marked point pattern.*

```{r}
#marked ppp using setmarks() and the "AccidentYear" attribute
bicycle_marks_ppp <- spatstat.geom::setmarks(bicycle_crd_ppp, factor(bicycle_accidents$AccidentYear))
```

```{r}
#Cross-K-function 2018 and 2019
# Compute K-function using Kest(), with edge correction.
bicycle_crd_kcross <- spatstat.explore::Gcross(bicycle_marks_ppp,i = "2018", j = "2019")

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd_kcross, main = "Cross--function for all bicycle accidents points 2018 and 2019")
```

```{r}
#Cross-K-function
# Compute K-function using Kest(), with edge correction.
bicycle_crd_kcross <- spatstat.explore::Gcross(bicycle_marks_ppp,i = "2018", j = "2021")

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd_kcross, main = "Cross-G-function for all bicycle accidents points 2018 and 2021")
```

## Task 17

*Discuss your results. Similarities and differences between the various point sets? Noteworthy spatial and/or temporal patterns? Is any difference observable in the patterns of the cross-X function in the transition to the Covid-19 pandemic? etc. etc. Note: Consider in your interpretation that the distance scale may change between years.*

| blabla
| 

## Task 18

*From the bicycle accidents data, choose at least two relevant variables that make sense being compared, e.g. two different accident types, severity levels, times of day, years etc. For these selected accidents, **compute the counts within the ‘statistical zones’** of Zurich and **use these counts to compute the Getis-Ord G\*-statistic** for each of your counts layers. Visualize your results appropriately.*

```{r}
stat_zones <- st_read("data/zh_stat_zones.gpkg")
```

```{r}
### Read boundaries of statistical neighborhoods of Zurich

# Definition of the specific URL of the WFS
wfs_url <- "https://www.ogd.stadt-zuerich.ch/wfs/geoportal/Statistische_Quartiere?SERVICE=WFS&REQUEST=GetCapabilities&VERSION=1.1.0"

# Parse URL into a list for easier addition of information to the URL
url <- httr::parse_url(wfs_url)

# Instruct the WFS what information it should return.
# url$query was previously empty and is now being filled.
url$query <- list(service = "WFS",
                   request = "GetFeature",
                   typename = "adm_statistische_quartiere_map",
                   outputFormat = "GeoJSON"
                 )

# Inverse of parse_url (builds a URL from a list object)
request <- httr::build_url(url)

# sf::st_read can directly read WFS data into a data frame.
# WFS data is in WGS84, hence we need to transform to Swiss CRS LV95.
quart_sf <- sf::st_read(request, quiet = TRUE) |>
                        sf::st_transform(., crs = crs_lv95)
```

```{r}
### Count overall accidents per neighborhood polygon
quart_sf <- quart_sf |>
  dplyr::mutate(acc_count = base::lengths(sf::st_intersects(quart_sf, bicycle_accidents)))

# Calculate also accident density per km^2 for choropleth map
quart_sf <- quart_sf |> 
  dplyr::mutate(acc_dens = 1e6 * acc_count / sf::st_area(quart_sf))
```

```{r}
### mapview package
check_pkg("mapview")

# Plot bike accident counts per neighborhood as proportional circle map, overlaid
# on a map of the Zurich districts.
# We can add together multiple map layer using the '+' sign.
# Note that the scaling of both the color ramp and the circle sizes
# is automatic, using reasonable defaults.
mapview::mapview(quart_sf, layer.name = "Districts", 
                 zcol = "kname", alpha.regions = 0.5) +
  mapview::mapview(sf::st_centroid(quart_sf), 
                 layer.name = "Accident counts", cex = "acc_count")
```

```{r}
#counts of two severity types (counts are needed for the Getis-Ord G-statistic)
bicycle_severities <- bicycle_accidents |>   
  group_by(AccidentSeverityCategory_en) |>
  filter(AccidentSeverityCategory_en == "Accident with light injuries" | AccidentSeverityCategory_en == "Accident with severe injuries" |
           AccidentSeverityCategory_en == "Accident with fatalities") |>
  summarize(count = n()) |>   
  st_drop_geometry()  
bicycle_severities
```

```{r}
# Counting accidents of severity type 'light' vs 'severe'

# Filter the bicycle accidents dataset
accidents_light <- bicycle_accidents %>%
  dplyr::filter(AccidentSeverityCategory_en == "Accident with light injuries")  

accidents_severe <- bicycle_accidents %>%
  dplyr::filter(AccidentSeverityCategory_en == "Accident with severe injuries")  

# Count the number of accidents of each type in each stat zone
accidents_light_count <- sf::st_intersects(quart_sf, accidents_light) %>%
  lengths()  # This gives the count of accidents within each statistical zone

accidents_severe_count <- sf::st_intersects(quart_sf, accidents_severe) %>%
  lengths()
  
# Add the counts to the stat_zones object
quart_sf$accidents_light_count <- accidents_light_count
quart_sf$accidents_severe_count <- accidents_severe_count

```

```{r}
# Define neighbours for each zone (Queen's case - sharing a border or vertex)
# This creates a spatial weights list based on shared boundaries (contiguity)
neighbours <- spdep::poly2nb(quart_sf)

# Create a spatial weights list object from neighbours
weights <- spdep::nb2listw(neighbours, style = "W")  # "W" means row-standardized weights

# Calculate the Getis-Ord G* statistic for both accident counts

# Before it seemed the column was no vector, now ensure the column is a numeric vector
quart_sf$accidents_light_count <- as.numeric(quart_sf$accidents_light_count)
g_star_light <- spdep::localG(quart_sf$accidents_light_count, listw = weights)

# Ensure the column is a numeric vector
quart_sf$accidents_severe_count <- as.numeric(quart_sf$accidents_severe_count)
g_star_severe <- spdep::localG(quart_sf$accidents_severe_count, listw = weights)

# Add G* statistics to the stat_zones dataframe
quart_sf$g_star_light <- g_star_light
quart_sf$g_star_severe <- g_star_severe

```

```{r}
# Plot Getis-Ord G* statistic for light accidents
tm_shape(quart_sf) +
  tm_borders() +
  tm_fill(col = "g_star_light", palette = "RdBu", style = "cont") +
  tm_layout(main.title = "Getis-Ord G* for Light Bike Accidents", title.position = c("left", "top"))

# Plot Getis-Ord G* statistic for severe bicycle accidents
tm_shape(quart_sf) +
  tm_borders() +
  tm_fill(col = "g_star_severe", palette = "RdBu", style = "cont") +
  tm_layout(main.title = "Getis-Ord G* for Severe Bike Accidents", title.position = c("left", "top"))

```

\`\`\`
