---
title: "DC1_Part1"
author: "Laura Bozzi and Sarina Sägesser"
format: html
editor: visual
---

# DC1 Assignment, Part 1 (Spatial Clustering)

## Fragen an Röbi/Peter:

-   gehören fragen in dcs schon zum essay teil und müssen separat geschrieben werden oder zsm gut?

-   wieviel muss geschrieben werden? eher task - paragraph oder seitenweise?

-   bis wann werden neue parts des DC1 eingeführt? (neue aufgaben)

-   kurzes feedback zu unserem zwischenstand? in etwa was sie sich vorgestellt haben??

-   **criteria for using KDE:** was ist gemeint? (evtl im buch –\> sind einfach kernel shape und bandwidth, nicht mehr)

## Notizen für uns

-   bike datensätze aufräumen: bereinigen alles beieinander beispielsweise

-   kommentare in unseren worten schreiben!

-   task 9 besser?

-   task 10, 11, 12, 13

-   für zwei verschieden aussehende jahre machen (bisher immer 18/19 oder nur 18)

-   12: 95% sachen anpassen & verstehen was anpassen, sodass gut ausschaut

```{r}
library(readr) 
library(tidyverse) 
library(tidyr) 
library(sf) 
library(dplyr)
library(ggplot2)
library(osmdata)
library(phonTools)
library(spdep)

if(!"dbscan" %in% rownames(installed.packages())) install.packages("dbscan")
library(dbscan)
if(!"factoextra" %in% rownames(installed.packages())) install.packages("dbscan")
library(factoextra)
if(!"spatstat" %in% rownames(installed.packages())) install.packages("dbscan")
library(spatstat)

```

```{r}
crs_lv03  <- 21781 
crs_lv95  <- 2056 
crs_wgs84 <- 4326
```

## Task 1:

*Download and import the road accident data from Open Data Zurich (ODZ) using an appropriate file format. If needed, transform to the Swiss projected CH1903+/LV95 coordinate system (EPSG = 2056).*

```{r}
accidents <- st_read("roadtrafficaccidentlocations.gpkg")
#head(accidents)
```

As the point dimension seems to have a Z coordinate, which is always 0, we remove it here already.

```{r}
# Remove the Z coordinate from the geometry column
accidents <- sf::st_zm(accidents)

# Check if the Z coordinate has been successfully removed
print(accidents)
```

```{r}
plot(accidents)
```

## Task 2:

*Report the following numbers in table(s):*

*a. Number of accidents by accident severity category*

```{r}
#a. Number of accidents by accident severity category 
accident_severity <- accidents |>   
  group_by(AccidentSeverityCategory_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_severity
```

*b. Number of accidents by accident type*

```{r}
#b. Number of accidents by accident type 
accident_type <- accidents |>   
  group_by(AccidentType_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_type
```

*c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively. And combinations thereof (pedestrian AND bicycle, pedestrian AND motorcycle etc.). Are there any accidents involving all three modes (pedestrian, bicycle, motorcycle)?*

```{r}
#c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively 

selected_involvements <- accidents |>   
  dplyr::select(AccidentInvolvingPedestrian, AccidentInvolvingBicycle, AccidentInvolvingMotorcycle)  

accident_involvements <- selected_involvements |>   
  dplyr::summarise(across(everything(), ~ sum(. == "true", na.rm = "true"))) %>%   
  bind_rows(selected_involvements %>%               
              summarise(across(everything(), ~ sum(. == "false", na.rm = "true"))))|>  
  st_drop_geometry()  

row.names(accident_involvements) <- c("true", "false")

accident_involvements
```

```{r}
#c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively 

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
all_three_involvements <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true" & 
         AccidentInvolvingMotorcycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

all_three_involvements

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
ped_bike_inv <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

ped_bike_inv

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
bike_moto_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

bike_moto_inv

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
moto_ped_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingPedestrian == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

moto_ped_inv
```

There seem to be no accidents where all three involvements (bike, motorcycle and pedestrian) were registered.

## Task 3:

*Generate a plot showing the temporal evolution of the number of accidents from 2011 to 2023. Label each year with the corresponding number of accidents. Choose a plot type that is suitable for this type of temporal data. Bonus: Show also the data for the bicycle accidents (cf. Task 4) in the same plot.*

```{r}
#accidents per year
accident_per_year <- accidents |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_total = n()) |>   
  st_drop_geometry()  
accident_per_year

#bicycle accidents per year
bicycle_accidents_per_year <- accidents |>
  filter(AccidentInvolvingBicycle == "true") |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_bicycle = n()) |>   
  st_drop_geometry()  
bicycle_accidents_per_year

#combine data
total_plus_bicycle_per_year = left_join(accident_per_year, bicycle_accidents_per_year, by = "AccidentYear")
total_plus_bicycle_per_year
```

```{r}
ggplot(accident_per_year, aes(x = AccidentYear, y= count_total))+
  geom_line(color = "steelblue", linewidth = 1) +         
  geom_point(color = "steelblue", size = 2) +         
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents") +
  theme_minimal() 

ggplot(total_plus_bicycle_per_year, aes(x = AccidentYear)) +
  geom_line(aes(y = count_total, color = "Total Accidents"), size = 1) + 
  geom_point(aes(y = count_total, color = "Total Accidents"), size = 2) +
  geom_line(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 1) +
  geom_point(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 2) +
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents",
       colour = 'Legend') +
  scale_color_manual(values = c("Total Accidents" = "navyblue", "Bicycle Accidents" = "brown")) +
  theme_minimal()
```

## Task 4:

*Select only those accidents that involved a bicycle. From now on, and for the remainder of DC1, we will restrict our analysis to the accidents involving bicycles. With this subset, produce a map showing the bicycle accident data colored by accident severity category. Use a basemap such as OpenStreetMap and/or the boundary data available on OLAT, so the accidents can be visually and spatially referenced.*

```{r}
library(httr2)
bicycle_accidents <- accidents |>
  filter(AccidentInvolvingBicycle == "true")

bbox <- st_bbox(bicycle_accidents)

# osm_basemap <- opq(bbox = bbox) |>
#   add_osm_feature(key="boundary", value = "administrative") %>%
#   add_osm_feature(key="admin_level", value = "8") %>%
#   osmdata_sf()
```

```{r}
zh_city_boundary <- st_read('data/Zurich_city_boundary_2024.gpkg')
zh_city_roads <- st_read('data/Strassennetz_-OGD.gpkg')
```

```{r}
ggplot() +
  geom_sf(data = zh_city_boundary) +
  #geom_sf(data = osm_basemap$osm_lines, color = "gray", size = 0.5, alpha = 0.7) +
  geom_sf(data = bicycle_accidents, aes(color = AccidentSeverityCategory_en), size = 1.5, alpha = 0.8) +
  labs(title = "Accidents on OpenStreetMap Basemap",
       subtitle = "Accidents plotted over streets from OSM",
       color = "Accident Severity") +
  coord_sf() +
  theme_minimal() 
```

## Task 5:

*Imagine you are given the task of detecting spatial clusters of elevated bicycle accident occurrence (without considering their severity). How would you characterize such “bicycle accident clusters”? Try to define properties that can be used to describe and identify such clusters, and that can be used to choose and parameterize a clustering method suitable for the task. Try to use natural, but precise and concise language in your answer.*

| We would be interested in knowing in what areas of Zurich the most bicycle accidents happen. On the generated map of all bicycle accident one can see that in the city center, close to the main train station there are many accidents. Such accident herds could therefore be detected with more security with a spatial density clustering.
| The accidents should occur within close proximity to one another, which indicates a spatial dependence. A threshold for maximum distance between accidents would be used in this case, as well as the minimum number of points within a cluster.
| 
| Parameters: Distance from one point to another and the minimum number of accidents within the defined distance radius to form a cluster.

## Task 6:

From the bicycle accidents, extract the years 2018 to 2021 and compute clusters for each year separately, using a clustering method you deem appropriate for the task, and choose the control parameters appropriately to capture the types of clusters you had in mind in your definition of Task 5. Justify your choice.

| We try to extract the clusters by using DBSCAN, as the method is a broadly used algorithm to detect spatial clusters. We apply this, because we have clusters of arbitrary shapes and large dataframes. Moreover, rather than looking at a certain attribute, we are interested in the spatial component, which can be analysed well with DBSCAN.

### Preliminary work

```{r}
bicycle_crd <- bicycle_accidents[, c("AccidentLocation_CHLV95_N", "AccidentLocation_CHLV95_E")]
```

```{r}
#bicycle_crd <- sf::st_coordinates(bicycle_accidents)
```

```{r}
bicycle_accidents_2018 <- bicycle_accidents |>
  filter(AccidentYear == "2018") 

bicycle_accidents_2019 <- bicycle_accidents |>
  filter(AccidentYear == "2019") 

bicycle_accidents_2020 <- bicycle_accidents |>
  filter(AccidentYear == "2020") 

bicycle_accidents_2021 <- bicycle_accidents |>
  filter(AccidentYear == "2021") 
```

```{r}
#keep only X and Y coordinates for the further analysis
bicycle_crd_2018 <- sf::st_coordinates(bicycle_accidents_2018) 

bicycle_crd_2019 <- sf::st_coordinates(bicycle_accidents_2019) 

bicycle_crd_2020 <- sf::st_coordinates(bicycle_accidents_2020) 

bicycle_crd_2021 <- sf::st_coordinates(bicycle_accidents_2021)
```

```{r}
#Found this function when looking for a way to find the number of clusters. Source: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust
fviz_nbclust(bicycle_crd_2021, kmeans, method = "silhouette")
```

### Bicycle accident clusters 2018

```{r}
#| label: dbscan-bicycle-accidents
# 2018
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2018, k = 3)

# We can somehow see a knee at about 0.009 km.
abline(h = 0.009, col = "red")

# So, let's also try 0.005 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db18_01 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.009, minPts = 3)
db18_01
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_01$cluster + 1,
     main = "DBSCAN result with eps = 0.009 km",
     asp = 1)

db18_02 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.004, minPts = 3)
db18_02
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db18_03 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.002, minPts = 3)
db18_03
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2019

```{r}
#| label: dbscan-bicycle-accidents
# 2019
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2019, k = 3)

# We can somehow see a knee at about 0.014 km.
abline(h = 0.01, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db19_01 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.01, minPts = 3)
db19_01
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_01$cluster + 1,
     main = "DBSCAN result with eps = 0.01 km",
     asp = 1)

db19_02 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.004, minPts = 3)
db19_02
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db19_03 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.002, minPts = 3)
db19_03
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2020

```{r}
#| label: dbscan-bicycle-accidents
# 2020
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2020, k = 3)

# We can somehow see a knee at about 0.014 km.
abline(h = 0.014, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 0.005, col = "blue")

# Now compute DBSCAN with different values for eps.
db20_01 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.014, minPts = 3)
db20_01
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_01$cluster + 1,
     main = "DBSCAN result with eps = 0.014 km",
     asp = 1)

db20_02 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.005, minPts = 3)
db20_02
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_02$cluster + 1,
     main = "DBSCAN result with eps = 0.005 km",
     asp = 1)

db20_03 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.0025, minPts = 3)
db20_03
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result with eps = 0.0025 km",
     asp = 1)
```

### Bicycle accident clusters 2021

```{r}
#| label: dbscan-bicycle-accidents
# 2021
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2021, k = 3)

# We can somehow see a knee at about 0.012 km.
abline(h = 0.012, col = "red")

# So, let's also try  0.004 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db21_01 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.012, minPts = 3)
db21_01
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_01$cluster + 1,
     main = "DBSCAN result with eps = 0.012 km",
     asp = 1)

db21_02 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.003, minPts = 3)
db21_02
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_02$cluster + 1,
     main = "DBSCAN result with eps = 0.003 km",
     asp = 1)

db21_03 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.002, minPts = 3)
db21_03
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Trying OPTICS algorithm

Here, the help of ChatGPT was used to get the OPTICS algorithm right.

```{r}
# First run with eps = 0.05 and minPts = 3
db <- dbscan::optics(bicycle_crd_2018, eps = 0.05, minPts = 3)
db
clusters <- dbscan::extractXi(db, xi = 0.05)
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.05, minPts = 3)",
     asp = 1)

# Second run with eps = 0.015 and minPts = 5
db <- dbscan::optics(bicycle_crd_2018, eps = 0.015, minPts = 5)
db
clusters <- dbscan::extractXi(db, xi = 0.05)  # Re-extract clusters for this run
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.015, minPts = 5)",
     asp = 1)

```

## Task 7:

Discuss your results, including also limitations or problems, and possible other methods that you could have used.

| In the cluster diagrams of the years 2018-2021 there is not too much variation from one year to another. What stands out in every diagram, at least the two more accurate ones (with the two smallest eps distance) show the kernel around the main train station nicely. We could say that the Zurich HB/ Central acts as a bicycle accident herd. This is no surprise as many tram lines pass by the Zurich HB, as well as that is very often crowded with cars, pedestrians and cyclists.
| 

| The OPTICS algorithm has been used as a comparison. However, another method which could have been used was the convex cluster hulls. In some parts of Zurich that might have been an addition to the straight forward analysis but for example around HB we would have no holes within the spaces of large density.
| 
| What can be said as limiting factors is that the streets were not included in the calculation. It would be interesting to know if most accidents happen on main or peripheral streets. Moreover, the time has not been included either. We could filter for the rush hour in a next step, in order to analyse the accidents during that time. There, we might have other herds than when looking at all the accidents.

## Task 8:

Given the clusters that you have extracted in Part 1 of the DC1 assignment: a. Define a set of criteria that a method should fulfill that can be used to delineate the given clusters by polygons. Use free text for these definitions, but try to be concise and precise. (Note: These criteria can also be used in the subsequent Discussion to evaluate whether they have been met.) b. Choose a polygon delineation method that you deem appropriate in light of the above criteria. Justify your choice.

### 8a)

| The method should be able to detect an area which is very dense in bicycle accidents. It should work kind of like a heat map, where we see a center where many accidents happen on the same spot and towards the edges there are less and less. Therefore, the polygon should be drawn at the border between "many accidents and less accidents".

### 8b)

| Based on the criteria above, we would chose the **Kernel Density Estimation (KDE) with bandwidth selection method HREF.** With this method, we could define clusters based on areas where KDE values exceed a certain threshold, indicating high accident concentration.
| As we don't have attributes which we want to use to cluster the accidents but only the densities on the map such a method would be suitable. However, we wonder if it's possible to generate this cluster estimation with more than one center.
| As an alternative, the method the Standard Deviation Ellipse will be tried out.
| 
| 

## Task 9:

From the years 2018 to 2021 for which you computed clusters in Task 6, choose at least two years and apply your polygon delineation method of choice to each of these two years separately. Compute the Jaccard Index (aka Intersection over Union) for pair(s) of selected years and present and discuss the results.

```{r}
# OPTICS with the extractXi() function on the data
#2018
pt_bike18_reach <- dbscan::optics(bicycle_crd_2018, eps = 1000.0, minPts = 3)
pt_bike18_reach <- dbscan::extractXi(pt_bike18_reach, xi = 0.05)
# Extract reachability distances from the OPTICS object
reachdist <- pt_bike18_reach$reachdist

# Plot reachability distances with specific ylim settings
plot(reachdist, type = "l", ylim = c(0, 0.015), xlab = "Points", ylab = "Reachability Distance",
     main = "Reachability Plot 2018")
 
dbscan::hullplot(bicycle_crd_2018, pt_bike18_reach, asp = 1, main = "Convex Hull Plot 2018")

#2019
pt_bike19_reach <- dbscan::optics(bicycle_crd_2019, eps = 1000.0, minPts = 3)
pt_bike19_reach <- dbscan::extractXi(pt_bike19_reach, xi = 0.05)
# Extract reachability distances from the OPTICS object
reachdist <- pt_bike19_reach$reachdist

# Plot reachability distances with specific ylim settings
plot(reachdist, type = "l", ylim = c(0, 0.015), xlab = "Points", ylab = "Reachability Distance",
     main = "Reachability Plot 2019")
 
dbscan::hullplot(bicycle_crd_2019, pt_bike19_reach, asp = 1, main = "Convex Hull Plot 2019")
```

## Task 9 (alternative):

```{r}

#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd_18 <- phonTools::sdellipse(bicycle_crd_2018, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd_18[,1], y = sde_crd_18[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2018[,1]), y = mean(sde_crd_18[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2018")

```

```{r}
#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd_19 <- phonTools::sdellipse(bicycle_crd_2019, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd_19[,1], y = sde_crd_19[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2019[,1]), y = mean(sde_crd_19[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2019")

```

### Jaccard Index:

```{r}
#In order to compute the jaccard index, the matrices have to become the same length, we therefore shorten all four years to the shortest of the four. 

# Find the minimum number of rows among all matrices
min_length <- min(nrow(bicycle_crd_2018), nrow(bicycle_crd_2019), nrow(bicycle_crd_2020), nrow(bicycle_crd_2021))

# Shorten all matrices to have the same number of rows as the smallest matrix
matrix18 <- bicycle_crd_2018[1:min_length, ]
matrix19 <- bicycle_crd_2019[1:min_length, ]
matrix20 <- bicycle_crd_2020[1:min_length, ]
matrix21 <- bicycle_crd_2021[1:min_length, ]

```

```{r}
#adehabitatHR::kerneloverlap()
install.packages("vegan")
library(vegan)
df_18_19 <-data.frame(matrix18, matrix19)
vegdist(df_18_19, method = "jaccard")

```

```{r}
#Found this jaccard index function here: https://www.r-bloggers.com/2021/11/how-to-calculate-jaccard-similarity-in-r-2/
#Define the function:
jaccard <- function(a, b) {
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}

#Compute the similarity between the matrices of bike accidents in 2018 and 2019
jaccard(matrix18, matrix19)
#Compute the similarity between the delineated polygons of bike accidents in 2018 and 2019 --> maybe this is better (nr makes more sense) than matrices?? I guess for jaccard you need a polygon geometry and not points but unsure
jaccard(pt_bike18_reach, pt_bike19_reach)
```

| A value of 0.25 between the years 2018 and 2019 shows that the delineated polygons overlap with each other indeed but not too much. 0 means no intersection at all, whereby 1 would mean one area is covered by another, thus, they would have the same clusters and the same sizes in the two analysed years. We don't have the latter case, which can also be observer in the convex hull plots at the start of task 9. We assume however, that the main clusters do overlap, which is what makes out the above jaccard index's score.

## Task 10:

Overall, what did you find with the above steps? What do these steps tell you about the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing the given data? Any other points of note?

Trying to plot four graphs next to each other

```{r}
# 4 figures arranged in 2 rows and 2 columns
par(mfrow = c(2, 2),
    mar = c(4, 4, 1, 1),  # Margin sizes: c(bottom, left, top, right)
    oma = c(1, 1, 2, 1))  # Outer margin sizes: c(bottom, left, top, right)


# Plot for the year 2018
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2018)",
     asp = 1)

# Plot for the year 2019
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2019)",
     asp = 1)

# Plot for the year 2020
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result eps = 0.0025 km (2020)",
     asp = 1)

# Plot for the year 2021
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2021)",
     asp = 1)

```

*Task 10: Overall, what did you find with the above steps? What do these steps tell you about the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing the given data? Any other points of note?*

| The above steps and graphs show us that the largest accident herd in the city of Zurich is between the upper end of lake Zurich and Letten. On all the graphs, a large blob of points can be seen. On the standard deviational ellipse we could also see this well.
| However, we cannot tell much about the other areas and streets around it as the point clusters don't seem like the best option to analyse such herds. This method with coloring the different clusters seems more adapt for datasets wich are to be clustered by classes. As we don't have classes here we want to distinguish but rather analyse densities, a kernel density estimation would be of more interest.

## Task 11

Similarly to the clustering and polygon delineation tasks carried out in Parts 1 and 2 of DC1, respectively, start off by defining **criteria for using KDE to detect areas/hotspots of elevated bicycle accident density**, and explain your reasoning.

Bandwidth selection and the kernel shape are the key criteria to define for using KDE.

Bandwidth h will be tested with HREF, LSCV as well as HPI. It would be interesting to use a weighted bandwidth, however, we don't have a value/location kind of column (like there is in the ungulates sample data). We tried to compute this but as no two accidents seem to be at the exact (important!) coordinates, it would have involved to create some kind of a buffer, then group and weigh the accidents per coordinates group... which would have been too time consuming.

We work with the bivariate normal kernel, which is the standard in kernelUD{adehabitatHR} and generally a popular option.

## Task 12

Choose any two years from the years 2018 to 2021 (justify your choice of years) and compute the KDE surfaces for each of these two separately and visualize your results. You are free to choose the KDE implementation (i.e., R package and function(s)) as well as the parameters (bandwidth selection method, etc.), but you should document your choices and discuss, in the subsequent Task 14, your results in light of your choices.

```{r}
#installing packages adehabitatHR and ks for KDE
if(!"adehabitatHR" %in% rownames(installed.packages())) install.packages("adehabitatHR")
library(adehabitatHR)
if(!"ks" %in% rownames(installed.packages())) install.packages("ks")
library(ks)
```

As the conversion from sf objects to sp needs a geometry column like POINT (X Y), so without the Z, we have to remove it

Remove the Z coordinate from the geometry column

bicycle_accidents_2018 \<- sf::st_zm(bicycle_accidents_2018)

Check if the Z coordinate has been successfully removed

print(bicycle_accidents_2018)

bicycle_accidents_2019 \<- sf::st_zm(bicycle_accidents_2019) bicycle_accidents_2020 \<- sf::st_zm(bicycle_accidents_2020) bicycle_accidents_2021 \<- sf::st_zm(bicycle_accidents_2021)

```{r}
#| label: kde-adehabitatHR
#| warning: false

# library(adehabitatHR)

ext_val <- 0.3    # 0.3
grid_val <- 300   # 300

# Convert the SF_PPOLYGON object to an SpatialPointsDataFrame object,
# because adehabitatHR wants SP objects as input.
# We use the as() function from the sf package.

# The inverse operation is done through `st_as_sf()` or `st_as_sfc()`, respectively. See the this [vignette](https://r-spatial.github.io/sf/articles/sf2.html) for examples (scroll to the bottom of the page).

bicycle_crds_18_sp <- as(bicycle_accidents_2018, "Spatial")

# First, use the reference bandwidth method for bandwidth selection (h = "href")
# Positioning of legend box is optimized for knitr HTML output

# Compute UD (utilization distribution)
ud <- adehabitatHR::kernelUD(bicycle_crds_18_sp, grid = grid_val, extent = ext_val, h = "href")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]",
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud@coords[,1])
xmax <- max(ud@coords[,1])
ymin <- min(ud@coords[,2])
ymax <- max(ud@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method HREF", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), 
       col = c("black", "blue"), lwd = c(3, 2), lty = c(4, 1), 
       inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with HREF (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with HREF (50 %): ", hr50$area, sep = "", "\n")
################################################################################

# Now we use the reference least-squares cross-validation method (h = "LSCV")
# Positioning of legend box is optimized for knitr HTML output
ud <- adehabitatHR::kernelUD(bicycle_crds_18_sp, grid = grid_val, extent = ext_val, h = "LSCV")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]", 
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud@coords[,1])
xmax <- max(ud@coords[,1])
ymin <- min(ud@coords[,2])
ymax <- max(ud@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method LSCV", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), col = c("black", "blue"), 
       lwd = c(3, 2), lty = c(4, 1), inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with LSCV (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with LSCV (50 %): ", hr50$area, sep = "", "\n")
```

| The problem of over-smoothing can be observed in the kernel density estimation with HREF, however, the estimation computed with least-squares cross validation (LSCV) is too crowded and thus not very useful to analyse our accident data. The KDE with HREF diagram is better to analyse the extend of the overall accidents, plus it shows where 50% of all the bike accidents (in the year 2018) were located. What is more, the 95% line also shows the outline of the lake and hill of Zurich, where expectedly no accidents happen.

```{r}
#Shorten the bicycle_accidents_2018 to the same length as matrix 18 has, namely 632 rows
# Shorten the sf table to the first 632 rows
bicycle_accidents_2018 <- bicycle_accidents_2018[1:632, ]

# Check if the sf table now has 632 rows
nrow(bicycle_accidents_2018)
```

```{r}
#| label: kde-ks
#| warning: false

# library(ks)

# First estimate a value for the bandwidth, using the "Plug-in" selector (Hpi).
# Needs plain coordinates matrix as input --> ung_crd
h <- ks::Hpi(x = matrix18)

# Now compute KDE; first without weights applied
fkde <- ks::kde(matrix18, H = h)

# Plotting utilizes the specialized S3 plot.kde() function
plot(fkde, display = "filled.contour2", 
     cont = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100),  # percentage contours
     main = "Unweighted KDE; plug-in bandwidth", asp = 1)
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)

# unweighted, with SCV (smoothed cross validation) bandwidth selector
h <- ks::Hscv(x = matrix18)
fkde <- ks::kde(matrix18, H = h)
plot(fkde, display = "persp",
     main = "Unweighted KDE; smoothed cross validation bandwidth (SCV)")
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)

# unweighted, with LSCV (least-squares cross validation) bandwidth selector
h <- ks::Hlscv(x = matrix18)
fkde <- ks::kde(matrix18, H = h)
plot(fkde, display = "image",
     main = "Unweighted KDE; least-squares cross validation bandwidth (LSCV)", asp = 1)
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)
```

```{r}
length(bicycle_accidents_2018$value) == nrow(matrix18)
```

## Task 13

Compute the “volume of intersection” (“VI”) between the KDE surfaces (utilization distributions) of the two years. Hint: There are different ways to do that, but the adehabitatHR package has functionality for that. How do the results correspond to those of Task 9 (Jaccard Index or IoU)?

```{r}
#theory:
#https://rdrr.io/cran/adehabitatHR/man/kerneloverlap.html

#kernel_18_VI <- kerneloverlap(df_18_19, method = "VI", percent = 95, conditional = FALSE)
#plot(kernel_18_VI)
```

```{r}
str(df_18_19)
```

```{r}
# Extract the coordinates from the sf object into a matrix
coords_matrix <- sf::st_coordinates(bicycle_accidents_2018)

# Display the column names of the coordinates matrix to verify
print(colnames(coords_matrix))

# Now select only the relevant columns (usually labeled as X and Y) for conversion
coords_matrix_xy <- coords_matrix[, c("X", "Y")]

# Convert the coordinates to a SpatialPoints object
spatial_points <- SpatialPoints(coords_matrix_xy, proj4string = CRS("+proj=longlat +datum=WGS84"))

head(spatial_points)

# Use the SpatialPoints object in the kerneloverlap function
#kernel_18_VI <- kerneloverlap(spatial_points, method = "VI", percent = 95, conditional = FALSE)
#this gives always the same error and Idk why?!

```

## Task 14

Discuss your results for this part of DC1 (density estimation). What did you find? Compare the results of this part with the clusters/polygons of Parts 1 and 2 (see note below): What are the commonalities? What are the differences? Which method(s) perform more ade- quately than others for the given problem and data? Which method(s) would you recommend, and which ones not? Why? (You are free to add more points to the discussion.)

| bla

## Task 15

Choose one or more distance measure functions (justify your choice) and compute it/them for a. all bicycle accidents (2011 - 2021) b. bicycle accidents for one of the years selected in Task 6 c. bicycle accidents for only the cluster points of the year selected in (b) Restrict your analysis to the “inner city” and use the same window for all point sets. It’s up to you to define the extent of the “inner city” and explain/justify what that means in terms of this data challenge.

### a)

```{r}

```

```{r}
X_crd <- as.numeric(bicycle_accidents$AccidentLocation_CHLV95_E)
Y_crd <- as.numeric(bicycle_accidents$AccidentLocation_CHLV95_N)

win_all <- as.owin(st_bbox(bicycle_accidents))
bicycle_crd_ppp <- spatstat.geom::ppp(x = X_crd, y = Y_crd, window = win_all, unitname = c("meter", "meters"))

summary(bicycle_crd_ppp)

# Compute K-function using Kest(), with edge correction.
bicycle_crd_gf <- spatstat.explore::envelope(bicycle_crd_ppp, fun = Gest, nsim = 100, correction = "best", verbose = FALSE)

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd_gf, main = "G-function for all bicycle accidents points")
```

### b)

```{r}
win <- as.owin(st_bbox(bicycle_accidents_2018))
bicycle_crd18_ppp <- spatstat.geom::ppp(x = bicycle_crd_2018[,1], y = bicycle_crd_2018[,2], window = win, unitname = c("meter", "meters"))
```

```{r}
#| label: K-fun
#| 
# Compute K-function using Kest(), with edge correction.
bicycle_crd18_gf <- spatstat.explore::envelope(bicycle_crd18_ppp, fun = Gest, nsim = 100, correction = "best", verbose = FALSE)

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd18_gf, main = "G-function for bicycle accidents 2018 points")
```

### c)

```{r}

```

```{r}
#not working yet - db18_02 has sf format and needs to be a matrix (maybe using normal crds 18 works?)
db18_02_ppp <- spatstat.geom::ppp(x = db18_02[,1], y = db18_02[,2], window = win, unitname = c("meter", "meters"))

# Compute K-function using Kest(), with edge correction.
db18_02_gf <- spatstat.explore::envelope(db18_02_ppp, fun = Gest, nsim = 100, correction = "best", verbose = FALSE)

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(db18_02_gf, main = "G-function for bicycle accidents 2018 points")
```

## Task 16

Now choose the following two pairs of years, 2018 & 2019, as well as 2018 & 2021, and compute the cross-X function, where “X” stands for the function(s) you used in Task 15. Use the AccidentYear as the marks to produce a marked point pattern.

```{r}
#marked ppp using setmarks() and the "AccidentYear" attribute
bicycle_marks_ppp <- spatstat.geom::setmarks(bicycle_crd_ppp, factor(bicycle_accidents$AccidentYear))
```

```{r}
#Cross-K-function 2018 and 2019
# Compute K-function using Kest(), with edge correction.
bicycle_crd_kcross <- spatstat.explore::Gcross(bicycle_marks_ppp,i = "2018", j = "2019")

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd_kcross, main = "Cross--function for all bicycle accidents points 2018 and 2019")
```

```{r}
#Cross-K-function
# Compute K-function using Kest(), with edge correction.
bicycle_crd_kcross <- spatstat.explore::Gcross(bicycle_marks_ppp,i = "2018", j = "2021")

# When plotting the result of K-, L-, G-functions, spatstat uses an S3 method plot.fv()
# that overwrites the basic plot() function and specializes it.
plot(bicycle_crd_kcross, main = "Cross-G-function for all bicycle accidents points 2018 and 2021")
```

## Task 17

Discuss your results. Similarities and differences between the various point sets? Noteworthy spatial and/or temporal patterns? Is any difference observable in the patterns of the cross-X function in the transition to the Covid-19 pandemic? etc. etc. Note: Consider in your interpretation that the distance scale may change between years.

```{r}

```

## Task 18

From the bicycle accidents data, choose at least two relevant variables that make sense being compared, e.g. two different accident types, severity levels, times of day, years etc. For these selected accidents, compute the counts within the ‘statistical zones’ of Zurich and use these counts to compute the Getis-Ord G\*-statistic for each of your counts layers. Visualize your results appropriately.

--\> ich nimme mal 2 accident types, mer chönds ja nachher au ändere wenn öppis anders schlauer erschiint.

```{r}
#counts of two severity types (counts are needed for the Getis-Ord G-statistic)
bicycle_severities <- bicycle_accidents |>   
  group_by(AccidentSeverityCategory_en) |>
  filter(AccidentSeverityCategory_en == "Accident with light injuries" | AccidentSeverityCategory_en == "Accident with severe injuries") |>
  summarize(count = n()) |>   
  st_drop_geometry()  
bicycle_severities

```

```{r}
#get rid of the z-coordinate in the geometry
bicycle_accidents <- st_zm(bicycle_accidents)
#coords_2d <- st_zm(st_geometry(bicycle_accidents))
```

```{r}
#| label: g-stat-weights
#| 
# library("spdep")

# Find nearest neighbors within 2 kilometers (= 2000 m).
# You should play with nn.dis and see what it does to connectivity.
nn_dis <- 2000

# We first call dnearneigh() with min. distance = 0 and max. distance = nn_dis.
# This creates an nb object, that is, a list of Euclidean neighbors for each 
# point in sads_comp_sp.
bicycle_accidents_nb <- spdep::dnearneigh(bicycle_accidents, 0, nn_dis)

# We then call nb2listw() to convert the nb object sads_nb_u into a 
# listw object sads_lw_u, which contains the spatial weights used to 
# compute the G-statistic. Style = 'B' means that binary weights are generated 
# (1 if inside nn_dis, 0 if outside nn_dis). Using the include.self() function
# includes the self-weights w_{ii} > 0, and thus later results in Gi* statistics
# being computed instead of simply G statistics.
bicycle_accidents_lw <- spdep::nb2listw(spdep::include.self(bicycle_accidents_nb), style = "B")

```

```{r}
#trying the same procedure with only one year bc the nb object is way too big
bicycle_accidents18_nb <- spdep::dnearneigh(bicycle_accidents_2018, 0, nn_dis)

bicycle_accidents18_lw <- spdep::nb2listw(spdep::include.self(bicycle_accidents18_nb), style = "B")
```

--\> de folgendi chunk lauft irgendwie nie richtig dure und bim abbreche chunnt folgende Fehler: Fehler in db18_02\[, 1\] : falsche Anzahl von Dimensionen. Kei Ahnig wieso aber irgendwie schiints verchnüpft zsii oder R isch wieder am abkacke bi mir. Obe bim Task 15c chunnt de Fehler mit de Dimensione bi mir au. Check nöd wieso

```{r}
summary(bicycle_accidents18_nb)
```

```{r}
# Create the line links from the nb object, requesting an sf object to be returned.
# CRS needs to be set as it was carried over to the nb object.

#bicycle_accidents_links <- spdep::nb2lines(bicycle_accidents_nb, coords = coords_2d, as_sf = TRUE)

bicycle_accidents_links <- spdep::nb2lines(bicycle_accidents_nb, coords = sf::st_geometry(bicycle_accidents), as_sf = TRUE) |> sf::st_set_crs(crs_lv95)

#bicycle_accidents_links <- st_set_crs(bicycle_accidents_links, crs_lv95)

# Plot the neighbor links using functions from the tmap package
tmap::tm_shape(cantons_bound) + tmap::tm_borders(col = "gray") +
  tmap::tm_shape(sads_links) + tmap::tm_lines(col = "red", lwd = 0.5) +
  tmap::tm_shape(bicycle_accidents) + tmap::tm_dots(col = "black", size = 0.1) +
  tmap::tm_scale_bar(position = c("right", "bottom"), width = 0.2) +
  tmap::tm_layout(
    title = paste("Neighbor links within ",
                  nn_dis / 1000, " km", sep = ""),
    title.position = c("left", "top"),
    title.size = 1.1,
    title.fontface = "bold",
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  )
```

Laura: Ich hans da mal nur mitem 2018 versuecht, wil d datemengi rieesig isch. Han statt sads_links d accident_links gno aber ka

```{r}
# Create the line links from the nb object, requesting an sf object to be returned.
# CRS needs to be set as it was carried over to the nb object.

#bicycle_accidents_links <- spdep::nb2lines(bicycle_accidents_nb, coords = coords_2d, as_sf = TRUE)

bicycle_accidents18_links <- spdep::nb2lines(bicycle_accidents18_nb, coords = sf::st_geometry(bicycle_accidents_2018), as_sf = TRUE) |> sf::st_set_crs(crs_lv95)

#bicycle_accidents_links <- st_set_crs(bicycle_accidents_links, crs_lv95)

# Plot the neighbor links using functions from the tmap package
tmap::tm_shape(zh_city_boundary) + tmap::tm_borders(col = "gray") +
  tmap::tm_shape(bicycle_accidents18_links) + tmap::tm_lines(col = "red", lwd = 0.5) +
  tmap::tm_shape(bicycle_accidents_2018) + tmap::tm_dots(col = "black", size = 0.1) +
  tmap::tm_scale_bar(position = c("right", "bottom"), width = 0.2) +
  tmap::tm_layout(
    title = paste("Neighbor links within ",
                  nn_dis / 1000, " km", sep = ""),
    title.position = c("left", "top"),
    title.size = 1.1,
    title.fontface = "bold",
    frame = FALSE,
    inner.margins = c(0.05, 0.05, 0.05, 0.05)
  )
```

## Task 19

Discuss your results. What did you find regarding the hot and cold spots in your accident count layers? How do they compare to each other across layers and across (past) methods? etc. Discuss also the influence of the parameter settings (e.g., neighbor search distance, spatial weight formation) on your results.

| blabla
| 
