---
title: "DC1_Part1"
author: "Laura Bozzi and Sarina Sägesser"
format: html
editor: visual
---

# DC1 Assignment, Part 1 (Spatial Clustering)

## Fragen an Röbi/Peter:

-   gehören fragen in dcs schon zum essay teil und müssen separat geschrieben werden oder zsm gut?

-   wieviel muss geschrieben werden? eher task - paragraph oder seitenweise?

-   bis wann werden neue parts des DC1 eingeführt? (neue aufgaben)

-   kurzes feedback zu unserem zwischenstand? in etwa was sie sich vorgestellt haben??

-   **criteria for using KDE:** was ist gemeint? (evtl im buch –\> sind einfach kernel shape und bandwidth, nicht mehr)

## Notizen für uns

-   bike datensätze aufräumen: bereinigen alles beieinander beispielsweise

-   kommentare in unseren worten schreiben!

-   task 10, 11, 12, 13

-   für zwei verschieden aussehende jahre machen (bisher immer 18/19 oder nur 18)

-   12: 95% sachen anpassen & verstehen was anpassen, sodass gut ausschaut

```{r}
library(readr) 
library(tidyverse) 
library(tidyr) 
library(sf) 
library(dplyr)
library(ggplot2)
library(osmdata)
library(phonTools)

if(!"dbscan" %in% rownames(installed.packages())) install.packages("dbscan")
library(dbscan)
if(!"factoextra" %in% rownames(installed.packages())) install.packages("dbscan")
library(factoextra)

```

```{r}
crs_lv03  <- 21781 
crs_lv95  <- 2056 
crs_wgs84 <- 4326
```

## Task 1:

```{r}
accidents <- st_read("roadtrafficaccidentlocations.json", crs = 4326)
#head(accidents)
```

```{r}
plot(accidents)
```

## Task 2:

```{r}
#a. Number of accidents by accident severity category 
accident_severity <- accidents |>   
  group_by(AccidentSeverityCategory_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_severity
```

```{r}
#b. Number of accidents by accident type 
accident_type <- accidents |>   
  group_by(AccidentType_de) |>   
  summarize(count = n()) |>   
  st_drop_geometry()  
accident_type
```

```{r}
#c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively 

# Still need to add the combinations of pedestrians, bicycles, and motorcycles
selected_involvements <- accidents |>   
  select(AccidentInvolvingPedestrian, AccidentInvolvingBicycle, AccidentInvolvingMotorcycle)  

accident_involvements <- selected_involvements |>   
  summarise(across(everything(), ~ sum(. == "true", na.rm = "true"))) %>%   
  bind_rows(selected_involvements %>%               
              summarise(across(everything(), ~ sum(. == "false", na.rm = "true"))))|>  
  st_drop_geometry()  

row.names(accident_involvements) <- c("true", "false")

accident_involvements
```

```{r}
#c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively 

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
all_three_involvements <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true" & 
         AccidentInvolvingMotorcycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

all_three_involvements

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
ped_bike_inv <- accidents |> 
  filter(AccidentInvolvingPedestrian == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

ped_bike_inv

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
bike_moto_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingBicycle == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

bike_moto_inv

# Count accidents involving all three categories (pedestrian, bicycle, motorcycle)
moto_ped_inv <- accidents |> 
  filter(AccidentInvolvingMotorcycle == "true" & 
         AccidentInvolvingPedestrian == "true") |>
  st_drop_geometry()  |>
  summarise(count = n())

moto_ped_inv
```

There seem to be no accidents where all three involvements (bike, motorcycle and pedestrian) were registered.

## Task 3:

```{r}
#accidents per year
accident_per_year <- accidents |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_total = n()) |>   
  st_drop_geometry()  
accident_per_year

#bicycle accidents per year
bicycle_accidents_per_year <- accidents |>
  filter(AccidentInvolvingBicycle == "true") |>
  mutate(AccidentYear = as.numeric(AccidentYear)) |>
  group_by(AccidentYear) |>   
  summarize(count_bicycle = n()) |>   
  st_drop_geometry()  
bicycle_accidents_per_year

#combine data
total_plus_bicycle_per_year = left_join(accident_per_year, bicycle_accidents_per_year, by = "AccidentYear")
total_plus_bicycle_per_year
```

```{r}
ggplot(accident_per_year, aes(x = accident_per_year$AccidentYear, y= accident_per_year$count_total))+
  geom_line(color = "steelblue", size = 1) +         
  geom_point(color = "steelblue", size = 2) +         
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents") +
  theme_minimal() 

ggplot(total_plus_bicycle_per_year, aes(x = AccidentYear)) +
  geom_line(aes(y = count_total, color = "Total Accidents"), size = 1) + 
  geom_point(aes(y = count_total, color = "Total Accidents"), size = 2) +
  geom_line(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 1) +
  geom_point(aes(y = count_bicycle, color = "Bicycle Accidents"), size = 2) +
  labs(title = "Temporal Evolution of Accidents from 2011 to 2023",
       x = "Year",
       y = "Number of Accidents",
       colour = 'Legend') +
  scale_color_manual(values = c("Total Accidents" = "navyblue", "Bicycle Accidents" = "brown")) +
  theme_minimal()
```

## Task 4:

```{r}
bicycle_accidents <- accidents |>
  filter(AccidentInvolvingBicycle == "true")

bbox <- st_bbox(bicycle_accidents)

osm_basemap <- opq(bbox = bbox) |>
  add_osm_feature(key="boundary", value = "administrative") %>%
  add_osm_feature(key="admin_level", value = "8") %>%
  osmdata_sf()
```

```{r}
zh_city_boundary <- st_read('data/Zurich_city_boundary_2024.gpkg')
```

```{r}
ggplot() +
  geom_sf(data = zh_city_boundary) +
  #geom_sf(data = osm_basemap$osm_lines, color = "gray", size = 0.5, alpha = 0.7) +
  geom_sf(data = bicycle_accidents, aes(color = bicycle_accidents$AccidentSeverityCategory_en), size = 1.5, alpha = 0.8) +
  labs(title = "Accidents on OpenStreetMap Basemap",
       subtitle = "Accidents plotted over streets from OSM",
       color = "Accident Severity") +
  coord_sf() +
  theme_minimal() 
```

## Task 5:

```{r}
#Imagine you are given the task of detecting spatial clusters of elevated bicycle accident occurrence (without considering their severity). How would you characterize such “bicycle accident clusters”? Try to define properties that can be used to describe and identify such clusters, and that can be used to choose and parameterize a clustering method suitable for the task. Try to use natural, but precise and concise language in your answer.
```

| We would be interested in knowing in what areas of Zurich the most bicycle accidents happen. On the generated map of all bicycle accident one can see that in the city center, close to the main train station there are many accidents. Such accident herds could therefore be detected with more security with a spatial density clustering.
| The accidents should occur within close proximity to one another, which indicates a spatial dependence. A threshold for maximum distance between accidents would be used in this case, as well as the minimum number of points within a cluster.
| 
| Parameters: Distance from one point to another and the minimum number of accidents within the defined distance radius to form a cluster.

## Task 6:

### Preliminary work

```{r}
bicycle_crd <- sf::st_coordinates(bicycle_accidents)
```

```{r}
bicycle_accidents_2018 <- bicycle_accidents |>
  filter(AccidentYear == "2018") 

bicycle_accidents_2019 <- bicycle_accidents |>
  filter(AccidentYear == "2019") 

bicycle_accidents_2020 <- bicycle_accidents |>
  filter(AccidentYear == "2020") 

bicycle_accidents_2021 <- bicycle_accidents |>
  filter(AccidentYear == "2021") 
```

```{r}
bicycle_crd_2018 <- sf::st_coordinates(bicycle_accidents_2018)
bicycle_crd_2018 <- bicycle_crd_2018[, !(colnames(bicycle_crd_2018) %in% "Z")]

bicycle_crd_2019 <- sf::st_coordinates(bicycle_accidents_2019)
bicycle_crd_2019 <- bicycle_crd_2019[, !(colnames(bicycle_crd_2019) %in% "Z")]

bicycle_crd_2020 <- sf::st_coordinates(bicycle_accidents_2020)
bicycle_crd_2020 <- bicycle_crd_2020[, !(colnames(bicycle_crd_2020) %in% "Z")]

bicycle_crd_2021 <- sf::st_coordinates(bicycle_accidents_2021)
bicycle_crd_2021 <- bicycle_crd_2021[, !(colnames(bicycle_crd_2021) %in% "Z")]
```

```{r}
#Found this function when looking for a way to find the number of clusters. Source: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust
fviz_nbclust(bicycle_crd_2021, kmeans, method = "silhouette")
```

### Bicycle accident clusters 2018

```{r}
#| label: dbscan-bicycle-accidents
# 2018
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2018, k = 3)

# We can somehow see a knee at about 0.009 km.
abline(h = 0.009, col = "red")

# So, let's also try 0.005 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db18_01 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.009, minPts = 3)
db18_01
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_01$cluster + 1,
     main = "DBSCAN result with eps = 0.009 km",
     asp = 1)

db18_02 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.004, minPts = 3)
db18_02
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db18_03 <- dbscan::dbscan(bicycle_crd_2018, eps = 0.002, minPts = 3)
db18_03
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2019

```{r}
#| label: dbscan-bicycle-accidents
# 2019
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2019, k = 3)

# We can somehow see a knee at about 0.014 km.
abline(h = 0.01, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db19_01 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.01, minPts = 3)
db19_01
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_01$cluster + 1,
     main = "DBSCAN result with eps = 0.01 km",
     asp = 1)

db19_02 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.004, minPts = 3)
db19_02
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_02$cluster + 1,
     main = "DBSCAN result with eps = 0.004 km",
     asp = 1)

db19_03 <- dbscan::dbscan(bicycle_crd_2019, eps = 0.002, minPts = 3)
db19_03
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Bicycle accident clusters 2020

```{r}
#| label: dbscan-bicycle-accidents
# 2020
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2020, k = 3)

# We can somehow see a knee at about 0.014 km.
abline(h = 0.014, col = "red")

# So, let's also try  0.005 km.
graphics::abline(h = 0.005, col = "blue")

# Now compute DBSCAN with different values for eps.
db20_01 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.014, minPts = 3)
db20_01
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_01$cluster + 1,
     main = "DBSCAN result with eps = 0.014 km",
     asp = 1)

db20_02 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.005, minPts = 3)
db20_02
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_02$cluster + 1,
     main = "DBSCAN result with eps = 0.005 km",
     asp = 1)

db20_03 <- dbscan::dbscan(bicycle_crd_2020, eps = 0.0025, minPts = 3)
db20_03
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result with eps = 0.0025 km",
     asp = 1)
```

### Bicycle accident clusters 2021

```{r}
#| label: dbscan-bicycle-accidents
# 2021
# ------------------------------------------------------------------------------
# We first draw the kNN distance plot, maintaining minPts = k = 3.
dbscan::kNNdistplot(bicycle_crd_2021, k = 3)

# We can somehow see a knee at about 0.012 km.
abline(h = 0.012, col = "red")

# So, let's also try  0.004 km.
graphics::abline(h = 0.004, col = "blue")

# Now compute DBSCAN with different values for eps.
db21_01 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.012, minPts = 3)
db21_01
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_01$cluster + 1,
     main = "DBSCAN result with eps = 0.012 km",
     asp = 1)

db21_02 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.003, minPts = 3)
db21_02
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_02$cluster + 1,
     main = "DBSCAN result with eps = 0.003 km",
     asp = 1)

db21_03 <- dbscan::dbscan(bicycle_crd_2021, eps = 0.002, minPts = 3)
db21_03
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result with eps = 0.002 km",
     asp = 1)
```

### Trying OPTICS algorithm

Here, the help of ChatGPT was used to get the OPTICS algorithm right.

```{r}
# First run with eps = 0.05 and minPts = 3
db <- dbscan::optics(bicycle_crd_2018, eps = 0.05, minPts = 3)
db
clusters <- dbscan::extractXi(db, xi = 0.05)
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.05, minPts = 3)",
     asp = 1)

# Second run with eps = 0.015 and minPts = 5
db <- dbscan::optics(bicycle_crd_2018, eps = 0.015, minPts = 5)
db
clusters <- dbscan::extractXi(db, xi = 0.05)  # Re-extract clusters for this run
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = clusters$cluster + 1,
     main = "OPTICS extracted clusters (eps = 0.015, minPts = 5)",
     asp = 1)

```

## Task 7:

Discuss your results, including also limitations or problems, and possible other methods that you could have used.

| In the cluster diagrams of the years 2018-2021 there is not too much variation from one year to another. What stands out in every diagram, at least the two more accurate ones (with the two smallest eps distance) show the kernel around the main train station nicely. We could say that the Zurich HB/ Central acts as a bicycle accident herd. This is no surprise as many tram lines pass by the Zurich HB, as well as that is very often crowded with cars, pedestrians and cyclists.
| 

| The OPTICS algorithm has been used as a comparison. However, another method which could have been used was the convex cluster hulls. In some parts of Zurich that might have been an addition to the straight forward analysis but for example around HB we would have no holes within the spaces of large density.
| 
| What can be said as limiting factors is that the streets were not included in the calculation. It would be interesting to know if most accidents happen on main or peripheral streets. Moreover, the time has not been included either. We could filter for the rush hour in a next step, in order to analyse the accidents during that time. There, we might have other herds than when looking at all the accidents.

## Task 8:

### 8a)

| The method should be able to detect an area which is very dense in bicycle accidents. It should work kind of like a heat map, where we see a center where many accidents happen on the same spot and towards the edges there are less and less. Therefore, the polygon should be drawn at the border between "many accidents and less accidents".

### 8b)

| Based on the criteria above, we would chose the **Kernel Density Estimation (KDE) with bandwidth selection method HREF.** With this method, we could define clusters based on areas where KDE values exceed a certain threshold, indicating high accident concentration.
| As we don't have attributes which we want to use to cluster the accidents but only the densities on the map such a method would be suitable. However, we wonder if it's possible to generate this cluster estimation with more than one center.
| As an alternative, the method the Standard Deviation Ellipse will be tried out.
| 
| 

## Task 9:

```{r}
# Let's also try OPTICS with the extractXi() function on the data
# of restaurants.
pt_bike18_reach <- dbscan::optics(bicycle_crd_2018, eps = 1000.0, minPts = 3)
pt_bike18_reach <- dbscan::extractXi(pt_bike18_reach, xi = 0.05)
# Extract reachability distances from the OPTICS object
reachdist <- pt_bike18_reach$reachdist

# Plot reachability distances with specific ylim settings
plot(reachdist, type = "l", ylim = c(0, 0.015), xlab = "Points", ylab = "Reachability Distance",
     main = "Reachability Plot")
 
dbscan::hullplot(bicycle_crd_2018, pt_bike18_reach, asp = 1)
```

## Task 9 (alternative):

```{r}

#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd <- phonTools::sdellipse(bicycle_crd_2018, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd[,1], y = sde_crd[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2018[,1]), y = mean(sde_crd[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2018")

```

```{r}
#| label: sde-phonTools

## Standard deviational ellipse (SDE) with phonTools::sdellipse()

# sdellipse() takes a matrix of x/y coordinates as input, as previously retrieved
# above from the SF object using sf::st_coordinates().

sde_crd <- phonTools::sdellipse(bicycle_crd_2019, stdev = 1, show = FALSE)

ggplot() +
  geom_polygon(aes(x = sde_crd[,1], y = sde_crd[,2]), 
               colour = "darkgreen", fill = "lightgreen", 
               linewidth = 0.8, alpha = 0.4) +
  geom_sf(data = bicycle_accidents_2018) +
  geom_point(aes(x = mean(bicycle_crd_2019[,1]), y = mean(sde_crd[,2])), 
                 colour = "red", size = 3) +
  coord_sf(datum = crs_wgs84) +
  xlab("Easting [m]") + ylab("Northing [m]") +    # label axes
  ggtitle("Standard Deviational Ellipse for Bicycle Accidents 2019")

```

### Jaccard Index:

```{r}
#In order to compute the jaccard index, the matrices have to become the same length, we therefore shorten all four years to the shortest of the four. 

# Find the minimum number of rows among all matrices
min_length <- min(nrow(bicycle_crd_2018), nrow(bicycle_crd_2019), nrow(bicycle_crd_2020), nrow(bicycle_crd_2021))

# Shorten all matrices to have the same number of rows as the smallest matrix
matrix18 <- bicycle_crd_2018[1:min_length, ]
matrix19 <- bicycle_crd_2019[1:min_length, ]
matrix20 <- bicycle_crd_2020[1:min_length, ]
matrix21 <- bicycle_crd_2021[1:min_length, ]

```

```{r}
#adehabitatHR::kerneloverlap()
install.packages("vegan")
library(vegan)
df<-data.frame(matrix18, matrix19)
vegdist(df, method = "jaccard")

```

```{r}
#Found this jaccard index function here: https://www.r-bloggers.com/2021/11/how-to-calculate-jaccard-similarity-in-r-2/
#Define the function:
jaccard <- function(a, b) {
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}

#Compute the similarity between the matrices of bike accidents in 2018 and 2019
jaccard(matrix18, matrix19)
```

## Task 10:

Trying to plot four graphs next to each other

```{r}
# 4 figures arranged in 2 rows and 2 columns
par(mfrow = c(2, 2),
    mar = c(4, 4, 1, 1),  # Margin sizes: c(bottom, left, top, right)
    oma = c(1, 1, 2, 1))  # Outer margin sizes: c(bottom, left, top, right)


# Plot for the year 2018
plot(bicycle_crd_2018, 
     cex = 0.5, pch = 19, col = db18_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2018)",
     asp = 1)

# Plot for the year 2019
plot(bicycle_crd_2019, 
     cex = 0.5, pch = 19, col = db19_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2019)",
     asp = 1)

# Plot for the year 2020
plot(bicycle_crd_2020, 
     cex = 0.5, pch = 19, col = db20_03$cluster + 1,
     main = "DBSCAN result eps = 0.0025 km (2020)",
     asp = 1)

# Plot for the year 2021
plot(bicycle_crd_2021, 
     cex = 0.5, pch = 19, col = db21_03$cluster + 1,
     main = "DBSCAN result eps = 0.002 km (2021)",
     asp = 1)

```

*Task 10: Overall, what did you find with the above steps? What do these steps tell you about the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing the given data? Any other points of note?*

| The above steps and graphs show us that the largest accident herd in the city of Zurich is between the upper end of lake Zurich and Letten. On all the graphs, a large blob of points can be seen. On the standard deviational ellipse we could also see this well.
| However, we cannot tell much about the other areas and streets around it as the point clusters don't seem like the best option to analyse such herds. This method with coloring the different clusters seems more adapt for datasets wich are to be clustered by classes. As we don't have classes here we want to distinguish but rather analyse denisties, a kernel density estimation would be of more interest.

## Task 11

Similarly to the clustering and polygon delineation tasks carried out in Parts 1 and 2 of DC1, respectively, start off by defining **criteria for using KDE to detect areas/hotspots of elevated bicycle accident density**, and explain your reasoning.

Bandwidth selection and the kernel shape are the key criteria to define for using KDE.

What else should we write here?!

...

## Task 12

Choose any two years from the years 2018 to 2021 (justify your choice of years) and compute the KDE surfaces for each of these two separately and visualize your results. You are free to choose the KDE implementation (i.e., R package and function(s)) as well as the parameters (bandwidth selection method, etc.), but you should document your choices and discuss, in the subsequent Task 14, your results in light of your choices.

```{r}
#installing packages adehabitatHR and ks for KDE
if(!"adehabitatHR" %in% rownames(installed.packages())) install.packages("adehabitatHR")
library(adehabitatHR)
if(!"ks" %in% rownames(installed.packages())) install.packages("ks")
library(ks)
```

```{r}
# As the conversion from sf objects to sp needs a geometry column like POINT (X Y), so without the Z, we have to remove it
# Remove the Z coordinate from the geometry column
bicycle_accidents_2018 <- sf::st_zm(bicycle_accidents_2018)

# Check if the Z coordinate has been successfully removed
print(bicycle_accidents_2018)

bicycle_accidents_2019 <- sf::st_zm(bicycle_accidents_2019)
bicycle_accidents_2020 <- sf::st_zm(bicycle_accidents_2020)
bicycle_accidents_2021 <- sf::st_zm(bicycle_accidents_2021)

```

```{r}
#| label: kde-adehabitatHR
#| warning: false

# library(adehabitatHR)

ext_val <- 0.3    # 0.3
grid_val <- 300   # 300

# Convert the SF_PPOLYGON object to an SpatialPointsDataFrame object,
# because adehabitatHR wants SP objects as input.
# We use the as() function from the sf package.

# The inverse operation is done through `st_as_sf()` or `st_as_sfc()`, respectively. See the this [vignette](https://r-spatial.github.io/sf/articles/sf2.html) for examples (scroll to the bottom of the page).

bicycle_crds_18_sp <- as(bicycle_accidents_2018, "Spatial")

# First, use the reference bandwidth method for bandwith selection (h = "href")
# Positioning of legend box is optimized for knitr HTML output


# Compute UD (utilization distribution)
ud <- adehabitatHR::kernelUD(bicycle_crds_18_sp, grid = grid_val, extent = ext_val, h = "href")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]",
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud@coords[,1])
xmax <- max(ud@coords[,1])
ymin <- min(ud@coords[,2])
ymax <- max(ud@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method HREF", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), 
       col = c("black", "blue"), lwd = c(3, 2), lty = c(4, 1), 
       inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with HREF (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with HREF (50 %): ", hr50$area, sep = "", "\n")

# Now use the reference least-squares cross-validation method (h = "LSCV")
# Positioning of legend box is optimized for knitr HTML output
ud <- adehabitatHR::kernelUD(bicycle_crds_18_sp, grid = grid_val, extent = ext_val, h = "LSCV")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]", 
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud@coords[,1])
xmax <- max(ud@coords[,1])
ymin <- min(ud@coords[,2])
ymax <- max(ud@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method LSCV", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), col = c("black", "blue"), 
       lwd = c(3, 2), lty = c(4, 1), inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with LSCV (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with LSCV (50 %): ", hr50$area, sep = "", "\n")
```

```{r}
#Shorten the bicycle_accidents_2018 to the same length as matrix 18 has, namely 632 rows
# Shorten the sf table to the first 632 rows
bicycle_accidents_2018 <- bicycle_accidents_2018[1:632, ]

# Check if the sf table now has 632 rows
nrow(bicycle_accidents_2018)
```

```{r}
#| label: kde-ks
#| warning: false

# library(ks)

# First estimate a value for the bandwidth, using the "Plug-in" selector (Hpi).
# Needs plain coordinates matrix as input --> ung_crd
h <- ks::Hpi(x = matrix18)

# Now compute KDE; first without weights applied
fkde <- ks::kde(matrix18, H = h)

# Plotting utilizes the specialized S3 plot.kde() function
plot(fkde, display = "filled.contour2", 
     cont = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100),  # percentage contours
     main = "Unweighted KDE; plug-in bandwidth", asp = 1)
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)

# unweighted, with SCV (smoothed cross validation) bandwidth selector
h <- ks::Hscv(x = matrix18)
fkde <- ks::kde(matrix18, H = h)
plot(fkde, display = "persp",
     main = "Unweighted KDE; smoothed cross validation bandwidth (SCV)")
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)

# unweighted, with LSCV (least-squares cross validation) bandwidth selector
h <- ks::Hlscv(x = matrix18)
fkde <- ks::kde(matrix18, H = h)
plot(fkde, display = "image",
     main = "Unweighted KDE; least-squares cross validation bandwidth (LSCV)", asp = 1)
plot(bicycle_accidents_2018, cex = 0.5, pch = 16, col = "black", add = TRUE)


##### This last part cannot be done as we don't have the column "value" (so nr of accidents per data point) in our table -- could be computed maybe??

# Now the same with weights applied (i.e. number of animals per data point)
#h <- ks::Hlscv(x = matrix18)   # Hlscv = least squares cross validation
#fkde_w <- ks::kde(matrix18, H = h, w = bicycle_accidents_2018$value)

# assign number of animals to tmp and normalize values
#tmp <- bicycle_accidents_2018$value
#tmp <- (tmp - min(tmp)) / (max(tmp) - min(tmp))

#plot(fkde_w, display = "image", 
#     main = "Weighted KDE (number of observations per point); LSCV", asp = 1)
#plot(bicycle_accidents_2018, cex = 4*tmp, pch = 20, col = "black", add = TRUE)
```

```{r}
length(bicycle_accidents_2018$value) == nrow(matrix18)
```

## Task 13

Compute the “volume of intersection” (“VI”) between the KDE surfaces (utilization distributions) of the two years. Hint: There are different ways to do that, but the adehabitatHR package has functionality for that. How do the results correspond to those of Task 9 (Jaccard Index or IoU)?

## Task 14

Discuss your results for this part of DC1 (density estimation). What did you find? Compare the results of this part with the clusters/polygons of Parts 1 and 2 (see note below): What are the commonalities? What are the differences? Which method(s) perform more ade- quately than others for the given problem and data? Which method(s) would you recommend, and which ones not? Why? (You are free to add more points to the discussion.)
